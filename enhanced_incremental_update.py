#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºå¢é‡æ›´æ–° - æ”¯æŒå°†å¢é‡ç»“æœåˆå¹¶åˆ°å…¨é‡æ•°æ®
"""

import json
import os
import logging
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import asdict

from search_api_client import SearchResult
from config import Config

logger = logging.getLogger(__name__)

class EnhancedIncrementalUpdate:
    """å¢å¼ºçš„å¢é‡æ›´æ–°å™¨ - æ”¯æŒåˆå¹¶åˆ°å…¨é‡æ•°æ®"""
    
    def __init__(self):
        self.data_dir = "data"
        self.buff_full_file = os.path.join(self.data_dir, "buff_full.json")
        self.youpin_full_file = os.path.join(self.data_dir, "youpin_full.json")
    
    def merge_search_results_to_full_data(self, search_results: Dict[str, List[SearchResult]]):
        """å°†æœç´¢ç»“æœåˆå¹¶åˆ°å…¨é‡æ•°æ®æ–‡ä»¶ä¸­"""
        
        # åˆå¹¶åˆ°Buffå…¨é‡æ•°æ®
        self._merge_to_buff_full_data(search_results.get('buff', []))
        
        # åˆå¹¶åˆ°æ‚ æ‚ æœ‰å“å…¨é‡æ•°æ®  
        self._merge_to_youpin_full_data(search_results.get('youpin', []))
    
    def _merge_to_buff_full_data(self, buff_results: List[SearchResult]):
        """åˆå¹¶åˆ°Buffå…¨é‡æ•°æ®"""
        if not buff_results or not os.path.exists(self.buff_full_file):
            return
        
        try:
            # è¯»å–ç°æœ‰å…¨é‡æ•°æ®
            with open(self.buff_full_file, 'r', encoding='utf-8') as f:
                full_data = json.load(f)
            
            existing_items = full_data.get('items', [])
            
            # åˆ›å»ºç°æœ‰å•†å“çš„ç´¢å¼• (æŒ‰IDå’Œhash_name)
            existing_index = {}
            for i, item in enumerate(existing_items):
                item_id = str(item.get('id', ''))
                hash_name = item.get('market_hash_name', '')
                
                if item_id:
                    existing_index[f"id_{item_id}"] = i
                if hash_name:
                    existing_index[f"hash_{hash_name}"] = i
            
            # åˆå¹¶æ–°çš„æœç´¢ç»“æœ
            updates_count = 0
            additions_count = 0
            
            for result in buff_results:
                # å°†SearchResultè½¬æ¢ä¸ºBuffæ•°æ®æ ¼å¼
                buff_item = {
                    'id': result.id,
                    'name': result.name,
                    'market_hash_name': result.hash_name,
                    'sell_min_price': str(result.price),
                    'goods_info': {
                        'icon_url': result.image_url
                    },
                    'last_updated': datetime.now().isoformat(),
                    'source': 'incremental_search'  # æ ‡è®°æ¥æº
                }
                
                # æŸ¥æ‰¾æ˜¯å¦å­˜åœ¨
                found_index = None
                if result.id:
                    found_index = existing_index.get(f"id_{result.id}")
                if found_index is None and result.hash_name:
                    found_index = existing_index.get(f"hash_{result.hash_name}")
                
                if found_index is not None:
                    # æ›´æ–°ç°æœ‰å•†å“
                    existing_items[found_index].update(buff_item)
                    updates_count += 1
                else:
                    # æ·»åŠ æ–°å•†å“
                    existing_items.append(buff_item)
                    additions_count += 1
            
            # æ›´æ–°å…ƒæ•°æ®
            metadata = full_data.get('metadata', {})
            metadata.update({
                'total_count': len(existing_items),
                'last_incremental_merge': datetime.now().isoformat(),
                'incremental_updates': updates_count,
                'incremental_additions': additions_count
            })
            
            # ä¿å­˜æ›´æ–°åçš„æ•°æ®
            full_data['metadata'] = metadata
            full_data['items'] = existing_items
            
            with open(self.buff_full_file, 'w', encoding='utf-8') as f:
                json.dump(full_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… Buffå…¨é‡æ•°æ®å·²æ›´æ–°: æ›´æ–°{updates_count}ä¸ª, æ–°å¢{additions_count}ä¸ª")
            
        except Exception as e:
            logger.error(f"åˆå¹¶Buffå…¨é‡æ•°æ®å¤±è´¥: {e}")
    
    def _merge_to_youpin_full_data(self, youpin_results: List[SearchResult]):
        """åˆå¹¶åˆ°æ‚ æ‚ æœ‰å“å…¨é‡æ•°æ®"""
        if not youpin_results or not os.path.exists(self.youpin_full_file):
            return
        
        try:
            # è¯»å–ç°æœ‰å…¨é‡æ•°æ®
            with open(self.youpin_full_file, 'r', encoding='utf-8') as f:
                full_data = json.load(f)
            
            existing_items = full_data.get('items', [])
            
            # åˆ›å»ºç°æœ‰å•†å“çš„ç´¢å¼•
            existing_index = {}
            for i, item in enumerate(existing_items):
                item_id = str(item.get('commodityId', ''))
                hash_name = item.get('commodityHashName', '')
                
                if item_id:
                    existing_index[f"id_{item_id}"] = i
                if hash_name:
                    existing_index[f"hash_{hash_name}"] = i
            
            # åˆå¹¶æ–°çš„æœç´¢ç»“æœ
            updates_count = 0
            additions_count = 0
            
            for result in youpin_results:
                # å°†SearchResultè½¬æ¢ä¸ºæ‚ æ‚ æœ‰å“æ•°æ®æ ¼å¼
                youpin_item = {
                    'commodityId': result.id,
                    'commodityName': result.name,
                    'commodityHashName': result.hash_name,
                    'price': result.price,
                    'commodityUrl': result.image_url,
                    'last_updated': datetime.now().isoformat(),
                    'source': 'incremental_search'  # æ ‡è®°æ¥æº
                }
                
                # æŸ¥æ‰¾æ˜¯å¦å­˜åœ¨
                found_index = None
                if result.id:
                    found_index = existing_index.get(f"id_{result.id}")
                if found_index is None and result.hash_name:
                    found_index = existing_index.get(f"hash_{result.hash_name}")
                
                if found_index is not None:
                    # æ›´æ–°ç°æœ‰å•†å“
                    existing_items[found_index].update(youpin_item)
                    updates_count += 1
                else:
                    # æ·»åŠ æ–°å•†å“
                    existing_items.append(youpin_item)
                    additions_count += 1
            
            # æ›´æ–°å…ƒæ•°æ®
            metadata = full_data.get('metadata', {})
            metadata.update({
                'total_count': len(existing_items),
                'last_incremental_merge': datetime.now().isoformat(),
                'incremental_updates': updates_count,
                'incremental_additions': additions_count
            })
            
            # ä¿å­˜æ›´æ–°åçš„æ•°æ®
            full_data['metadata'] = metadata
            full_data['items'] = existing_items
            
            with open(self.youpin_full_file, 'w', encoding='utf-8') as f:
                json.dump(full_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… æ‚ æ‚ æœ‰å“å…¨é‡æ•°æ®å·²æ›´æ–°: æ›´æ–°{updates_count}ä¸ª, æ–°å¢{additions_count}ä¸ª")
            
        except Exception as e:
            logger.error(f"åˆå¹¶æ‚ æ‚ æœ‰å“å…¨é‡æ•°æ®å¤±è´¥: {e}")
    
    def get_merge_status(self) -> Dict:
        """è·å–åˆå¹¶çŠ¶æ€"""
        status = {
            'buff_full_exists': os.path.exists(self.buff_full_file),
            'youpin_full_exists': os.path.exists(self.youpin_full_file),
            'buff_last_merge': None,
            'youpin_last_merge': None,
            'buff_incremental_stats': {},
            'youpin_incremental_stats': {}
        }
        
        # æ£€æŸ¥Buffæ–‡ä»¶çŠ¶æ€
        if status['buff_full_exists']:
            try:
                with open(self.buff_full_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                metadata = data.get('metadata', {})
                status['buff_last_merge'] = metadata.get('last_incremental_merge')
                status['buff_incremental_stats'] = {
                    'updates': metadata.get('incremental_updates', 0),
                    'additions': metadata.get('incremental_additions', 0)
                }
            except Exception as e:
                logger.error(f"è¯»å–BuffçŠ¶æ€å¤±è´¥: {e}")
        
        # æ£€æŸ¥æ‚ æ‚ æœ‰å“æ–‡ä»¶çŠ¶æ€
        if status['youpin_full_exists']:
            try:
                with open(self.youpin_full_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                metadata = data.get('metadata', {})
                status['youpin_last_merge'] = metadata.get('last_incremental_merge')
                status['youpin_incremental_stats'] = {
                    'updates': metadata.get('incremental_updates', 0),
                    'additions': metadata.get('incremental_additions', 0)
                }
            except Exception as e:
                logger.error(f"è¯»å–æ‚ æ‚ æœ‰å“çŠ¶æ€å¤±è´¥: {e}")
        
        return status

# æµ‹è¯•åŠŸèƒ½
async def test_enhanced_incremental_update():
    """æµ‹è¯•å¢å¼ºçš„å¢é‡æ›´æ–°"""
    print("ğŸ§ª æµ‹è¯•å¢å¼ºå¢é‡æ›´æ–°")
    print("="*50)
    
    # æµ‹è¯•åˆå¹¶åŠŸèƒ½
    enhancer = EnhancedIncrementalUpdate()
    merge_status = enhancer.get_merge_status()
    
    print(f"å…¨é‡æ•°æ®æ–‡ä»¶çŠ¶æ€:")
    print(f"  Buffæ–‡ä»¶å­˜åœ¨: {merge_status['buff_full_exists']}")
    print(f"  æ‚ æ‚ æœ‰å“æ–‡ä»¶å­˜åœ¨: {merge_status['youpin_full_exists']}")
    
    if merge_status['buff_last_merge']:
        print(f"  ä¸Šæ¬¡Buffåˆå¹¶: {merge_status['buff_last_merge']}")
        print(f"  Buffå¢é‡ç»Ÿè®¡: {merge_status['buff_incremental_stats']}")
    if merge_status['youpin_last_merge']:
        print(f"  ä¸Šæ¬¡æ‚ æ‚ æœ‰å“åˆå¹¶: {merge_status['youpin_last_merge']}")
        print(f"  æ‚ æ‚ æœ‰å“å¢é‡ç»Ÿè®¡: {merge_status['youpin_incremental_stats']}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_enhanced_incremental_update()) 