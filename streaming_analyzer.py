#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµå¼ä»·å·®åˆ†æå™¨ - è¾¹è·å–è¾¹åˆ†æï¼Œå®æ—¶è¿”å›ç»“æœ
æ”¯æŒå¢é‡æ›´æ–°ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼Œé›†æˆå…¨å±€å¹¶å‘æ§åˆ¶
"""

import asyncio
import json
import time
from datetime import datetime
from typing import List, Dict, Optional, AsyncGenerator, Callable, Any
from dataclasses import asdict
import logging

from integrated_price_system import PriceDiffItem, BuffAPIClient
from optimized_api_client import OptimizedBuffClient, OptimizedYoupinClient
from config import Config
from analysis_manager import get_analysis_manager

logger = logging.getLogger(__name__)

class StreamingAnalyzer:
    """æµå¼ä»·å·®åˆ†æå™¨ - é›†æˆå…¨å±€å¹¶å‘æ§åˆ¶"""
    
    def __init__(self, 
                 progress_callback: Optional[Callable] = None,
                 result_callback: Optional[Callable] = None):
        """
        åˆå§‹åŒ–æµå¼åˆ†æå™¨
        
        Args:
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            result_callback: ç»“æœå›è°ƒå‡½æ•°
        """
        self.progress_callback = progress_callback
        self.result_callback = result_callback
        
        # ç¼“å­˜
        self.result_cache: List[PriceDiffItem] = []
        self.buff_cache: Dict[str, Any] = {}
        self.youpin_cache: Dict[str, float] = {}  # hash_name -> price
        self.youpin_name_cache: Dict[str, float] = {}  # name -> price
        
        # çŠ¶æ€è¿½è¸ª
        self.is_running = False
        self.total_processed = 0
        self.total_found = 0
        
    async def start_streaming_analysis(self) -> AsyncGenerator[Dict, None]:
        """å¯åŠ¨æµå¼åˆ†æ - é›†æˆå¹¶å‘æ§åˆ¶"""
        manager = get_analysis_manager()
        analysis_id = f"streaming_{int(time.time())}"
        
        # å°è¯•å¯åŠ¨åˆ†æ
        if not manager.start_analysis('streaming', analysis_id):
            yield {
                'type': 'error',
                'error': 'å·²æœ‰åˆ†æåœ¨è¿è¡Œï¼Œè¯·ç¨åå†è¯•',
                'message': f'å½“å‰æ­£åœ¨è¿è¡Œ: {manager.current_analysis_type}'
            }
            return
        
        try:
            self.is_running = True
            self.total_processed = 0
            self.total_found = 0
            
            # 1. é¦–å…ˆè¿”å›ç¼“å­˜æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
            cached_results = manager.get_cached_results()
            if cached_results:
                yield {
                    'type': 'cached_data',
                    'data': cached_results,
                    'message': f'è¿”å›ç¼“å­˜æ•°æ®: {len(cached_results)}ä¸ªå•†å“',
                    'cached': True,
                    'timestamp': datetime.now().isoformat()
                }
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
            if manager.should_stop():
                yield {
                    'type': 'cancelled',
                    'message': 'åˆ†æè¢«å–æ¶ˆ'
                }
                return
            
            # 2. å¼€å§‹å¹¶è¡Œè·å–æ•°æ®
            yield {
                'type': 'progress',
                'message': 'å¼€å§‹å¹¶è¡Œè·å–Buffå’Œæ‚ æ‚ æœ‰å“æ•°æ®...',
                'stage': 'data_fetching',
                'progress': 0
            }
            
            # ğŸ”¥ ä¿®å¤ï¼šé¦–å…ˆè·å–æ‚ æ‚ æœ‰å“æ•°æ®æ„å»ºæ˜ å°„è¡¨
            async for progress_info in self._stream_youpin_data():
                # å®šæœŸæ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
                if manager.should_stop():
                    yield {
                        'type': 'cancelled',
                        'message': 'åˆ†æè¢«å–æ¶ˆ'
                    }
                    return
                    
                yield progress_info
                if progress_info.get('type') == 'mapping_ready':
                    break
            
            # 3. å¼€å§‹æµå¼åˆ†æBuffæ•°æ®
            yield {
                'type': 'progress',
                'message': 'å¼€å§‹æµå¼åˆ†æBuffå•†å“...',
                'stage': 'analyzing',
                'progress': 0
            }
            
            # ğŸ”¥ ä¿®å¤ï¼šæµå¼å¤„ç†Buffæ•°æ®
            batch_results = []
            async for progress_info in self._stream_buff_data():
                # å®šæœŸæ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
                if manager.should_stop():
                    yield {
                        'type': 'cancelled',
                        'message': 'åˆ†æè¢«å–æ¶ˆ'
                    }
                    return
                    
                if progress_info.get('type') == 'data_batch':
                    # åˆ†æè¿™æ‰¹æ•°æ®
                    buff_items = progress_info['data']
                    batch_diff_items = await self._analyze_batch(buff_items)
                    
                    if batch_diff_items:
                        batch_results.extend(batch_diff_items)
                        self.result_cache.extend(batch_diff_items)
                        
                        # å®æ—¶è¿”å›åˆ†æç»“æœ
                        yield {
                            'type': 'incremental_results',
                            'data': [asdict(item) for item in batch_diff_items],
                            'batch_size': len(batch_diff_items),
                            'total_found': len(self.result_cache),
                            'total_processed': self.total_processed,
                            'message': f'æ–°å‘ç° {len(batch_diff_items)} ä¸ªä»·å·®å•†å“'
                        }
                
                elif progress_info.get('type') == 'progress':
                    yield progress_info
            
            # 4. æœ€ç»ˆç»“æœ
            final_results = [asdict(item) for item in self.result_cache]
            yield {
                'type': 'completed',
                'data': final_results,
                'total_found': len(self.result_cache),
                'total_processed': self.total_processed,
                'message': f'åˆ†æå®Œæˆï¼å…±å‘ç° {len(self.result_cache)} ä¸ªä»·å·®å•†å“',
                'timestamp': datetime.now().isoformat()
            }
            
            # æ›´æ–°ç®¡ç†å™¨ç¼“å­˜
            manager.finish_analysis(analysis_id, final_results)
            
        except Exception as e:
            logger.error(f"æµå¼åˆ†æå‡ºé”™: {e}")
            yield {
                'type': 'error',
                'error': str(e),
                'message': 'åˆ†æè¿‡ç¨‹å‡ºç°é”™è¯¯'
            }
            manager.finish_analysis(analysis_id)
        finally:
            self.is_running = False
    
    async def _stream_buff_data(self) -> AsyncGenerator[Dict, None]:
        """æµå¼è·å–Buffæ•°æ®"""
        manager = get_analysis_manager()
        
        try:
            async with OptimizedBuffClient() as client:
                # ğŸ”¥ å¦‚æœåˆ†æè¢«å–æ¶ˆï¼Œç«‹å³å–æ¶ˆå®¢æˆ·ç«¯
                if manager.should_stop():
                    client.cancel()
                    return
                # è·å–ç¬¬ä¸€é¡µç¡®å®šæ€»æ•°
                first_page = await client.get_goods_list(page_num=1)
                if not first_page or 'data' not in first_page:
                    raise Exception("æ— æ³•è·å–Buffç¬¬ä¸€é¡µæ•°æ®")
                
                first_data = first_page['data']
                total_pages = min(first_data.get('total_page', 0), Config.BUFF_MAX_PAGES)
                
                yield {
                    'type': 'progress',
                    'message': f'Buffæ€»å…±{total_pages}é¡µï¼Œå¼€å§‹é€é¡µè·å–...',
                    'stage': 'buff_fetching',
                    'total_pages': total_pages,
                    'current_page': 1
                }
                
                # å¤„ç†ç¬¬ä¸€é¡µ
                first_items = first_data.get('items', [])
                if first_items:
                    yield {
                        'type': 'data_batch',
                        'data': first_items,
                        'page': 1,
                        'total_pages': total_pages
                    }
                
                # è·å–å‰©ä½™é¡µé¢
                for page_num in range(2, total_pages + 1):
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
                    if not self.is_running or manager.should_stop():
                        logger.info(f"Buffæ•°æ®è·å–è¢«åœæ­¢ï¼Œå·²å¤„ç†{page_num-1}é¡µ")
                        client.cancel()  # ğŸ”¥ å–æ¶ˆå®¢æˆ·ç«¯
                        break
                        
                    page_data = await client.get_goods_list(page_num=page_num)
                    
                    if page_data and 'data' in page_data:
                        items = page_data['data'].get('items', [])
                        if items:
                            yield {
                                'type': 'data_batch',
                                'data': items,
                                'page': page_num,
                                'total_pages': total_pages
                            }
                    
                    # æŠ¥å‘Šè¿›åº¦
                    if page_num % 10 == 0:
                        yield {
                            'type': 'progress',
                            'message': f'Buffæ•°æ®è·å–è¿›åº¦: {page_num}/{total_pages}é¡µ',
                            'stage': 'buff_fetching',
                            'progress': (page_num / total_pages) * 100,
                            'current_page': page_num,
                            'total_pages': total_pages
                        }
                        
        except Exception as e:
            logger.error(f"Buffæ•°æ®è·å–å‡ºé”™: {e}")
            yield {
                'type': 'error',
                'error': f'Buffæ•°æ®è·å–å¤±è´¥: {str(e)}'
            }
    
    async def _stream_youpin_data(self) -> AsyncGenerator[Dict, None]:
        """æµå¼è·å–æ‚ æ‚ æœ‰å“æ•°æ®å¹¶æ„å»ºæ˜ å°„è¡¨"""
        manager = get_analysis_manager()
        
        try:
            async with OptimizedYoupinClient() as client:
                max_pages = Config.YOUPIN_MAX_PAGES
                
                yield {
                    'type': 'progress',
                    'message': f'å¼€å§‹è·å–æ‚ æ‚ æœ‰å“æ•°æ®ï¼Œæœ€å¤§{max_pages}é¡µ...',
                    'stage': 'youpin_fetching',
                    'progress': 0
                }
                
                # é€é¡µè·å–å¹¶æ„å»ºæ˜ å°„
                for page_index in range(1, max_pages + 1):
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
                    if not self.is_running or manager.should_stop():
                        logger.info(f"æ‚ æ‚ æœ‰å“æ•°æ®è·å–è¢«åœæ­¢ï¼Œå·²å¤„ç†{page_index-1}é¡µ")
                        client.cancel()  # ğŸ”¥ å–æ¶ˆå®¢æˆ·ç«¯
                        break
                        
                    items = await client.get_market_goods_safe(page_index=page_index)
                    
                    if items:
                        # æ„å»ºæ˜ å°„è¡¨
                        for item in items:
                            hash_name = item.get('commodityHashName', '')
                            commodity_name = item.get('commodityName', '')
                            price = item.get('price', 0)
                            
                            try:
                                price_float = float(price) if price else None
                                if price_float:
                                    if hash_name:
                                        self.youpin_cache[hash_name] = price_float
                                    if commodity_name:
                                        self.youpin_name_cache[commodity_name] = price_float
                            except (ValueError, TypeError):
                                continue
                        
                        # æŠ¥å‘Šè¿›åº¦
                        if page_index % 10 == 0:
                            yield {
                                'type': 'progress',
                                'message': f'æ‚ æ‚ æœ‰å“æ˜ å°„æ„å»º: {page_index}é¡µï¼Œç´¯è®¡{len(self.youpin_cache)}ä¸ªHashæ˜ å°„',
                                'stage': 'youpin_mapping',
                                'progress': (page_index / max_pages) * 100,
                                'hash_count': len(self.youpin_cache),
                                'name_count': len(self.youpin_name_cache)
                            }
                    else:
                        # å¦‚æœè·å–å¤±è´¥æˆ–æ— æ•°æ®ï¼Œç»“æŸè·å–
                        break
                
                # æ˜ å°„è¡¨æ„å»ºå®Œæˆ
                yield {
                    'type': 'mapping_ready',
                    'message': f'æ‚ æ‚ æœ‰å“æ˜ å°„è¡¨æ„å»ºå®Œæˆ: {len(self.youpin_cache)}ä¸ªHashæ˜ å°„, {len(self.youpin_name_cache)}ä¸ªåç§°æ˜ å°„',
                    'hash_count': len(self.youpin_cache),
                    'name_count': len(self.youpin_name_cache)
                }
                
        except Exception as e:
            logger.error(f"æ‚ æ‚ æœ‰å“æ•°æ®è·å–å‡ºé”™: {e}")
            yield {
                'type': 'error',
                'error': f'æ‚ æ‚ æœ‰å“æ•°æ®è·å–å¤±è´¥: {str(e)}'
            }
    
    async def _analyze_batch(self, buff_items: List[Dict]) -> List[PriceDiffItem]:
        """åˆ†æä¸€æ‰¹Buffå•†å“"""
        diff_items = []
        
        # åˆ›å»ºä¸´æ—¶BuffAPIClientç”¨äºè§£æ
        buff_client = BuffAPIClient()
        
        for item_data in buff_items:
            self.total_processed += 1
            
            # è§£æBuffå•†å“
            buff_item = buff_client.parse_goods_item(item_data)
            if not buff_item:
                continue

            # ğŸ”¥ æ£€æŸ¥Buffä»·æ ¼æ˜¯å¦åœ¨ç­›é€‰èŒƒå›´å†…
            if not Config.is_buff_price_in_range(buff_item.buff_price):
                continue

            # æŸ¥æ‰¾æ‚ æ‚ æœ‰å“ä»·æ ¼
            youpin_price = None
            matched_by = None
            
            # 1. Hashç²¾ç¡®åŒ¹é…
            if buff_item.hash_name and buff_item.hash_name in self.youpin_cache:
                youpin_price = self.youpin_cache[buff_item.hash_name]
                matched_by = "Hashç²¾ç¡®åŒ¹é…"
            
            # 2. åç§°ç²¾ç¡®åŒ¹é…ï¼ˆå¤‡ç”¨ï¼‰
            elif buff_item.name in self.youpin_name_cache:
                youpin_price = self.youpin_name_cache[buff_item.name]
                matched_by = "åç§°ç²¾ç¡®åŒ¹é…"
            
            if not youpin_price:
                continue
            
            # è®¡ç®—ä»·å·®
            price_diff = youpin_price - buff_item.buff_price
            if buff_item.buff_price > 0:
                profit_rate = (price_diff / buff_item.buff_price) * 100
            else:
                profit_rate = 0
            
            # æ£€æŸ¥æ˜¯å¦ç¬¦åˆä»·å·®åŒºé—´
            if Config.is_price_diff_in_range(price_diff):
                self.total_found += 1
                
                diff_item = PriceDiffItem(
                    id=buff_item.id,
                    name=buff_item.name,
                    buff_price=buff_item.buff_price,
                    youpin_price=youpin_price,
                    price_diff=price_diff,
                    profit_rate=profit_rate,
                    buff_url=buff_item.buff_url,
                    youpin_url=f"https://www.youpin898.com/search?keyword={buff_item.name}",
                    image_url=buff_item.image_url,
                    category=buff_item.category,
                    last_updated=datetime.now()
                )
                
                diff_items.append(diff_item)
        
        return diff_items
    
    def get_current_results(self) -> List[PriceDiffItem]:
        """è·å–å½“å‰åˆ†æç»“æœ"""
        return self.result_cache.copy()
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        self.result_cache.clear()
        self.buff_cache.clear()
        self.youpin_cache.clear()
        self.youpin_name_cache.clear()
        self.total_processed = 0
        self.total_found = 0
    
    def stop_analysis(self):
        """åœæ­¢åˆ†æ"""
        self.is_running = False

# ä½¿ç”¨ç¤ºä¾‹
async def test_streaming_analyzer():
    """æµ‹è¯•æµå¼åˆ†æå™¨"""
    print("ğŸ¯ æµ‹è¯•æµå¼ä»·å·®åˆ†æå™¨")
    print("="*50)
    
    def progress_callback(info):
        print(f"ğŸ“Š è¿›åº¦: {info.get('message', '')}")
    
    def result_callback(diff_items):
        print(f"ğŸ¯ å‘ç°ä»·å·®å•†å“: {len(diff_items)}ä¸ª")
    
    analyzer = StreamingAnalyzer(
        progress_callback=progress_callback,
        result_callback=result_callback
    )
    
    async for update in analyzer.start_streaming_analysis():
        update_type = update.get('type')
        message = update.get('message', '')
        
        if update_type == 'cached_data':
            print(f"ğŸ’¾ ç¼“å­˜æ•°æ®: {len(update['data'])}ä¸ªå•†å“")
        elif update_type == 'progress':
            progress = update.get('progress', 0)
            print(f"ğŸ“ˆ {message} ({progress:.1f}%)")
        elif update_type == 'incremental_results':
            batch_size = update.get('batch_size', 0)
            total_found = update.get('total_found', 0)
            print(f"âœ… å¢é‡ç»“æœ: +{batch_size}ä¸ª, æ€»è®¡: {total_found}ä¸ª")
        elif update_type == 'completed':
            total_found = update.get('total_found', 0)
            print(f"ğŸ‰ åˆ†æå®Œæˆ: å…±å‘ç°{total_found}ä¸ªä»·å·®å•†å“")
            break
        elif update_type == 'error':
            print(f"âŒ é”™è¯¯: {update.get('error', '')}")
            break

if __name__ == "__main__":
    asyncio.run(test_streaming_analyzer()) 