#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ›´æ–°ç®¡ç†å™¨ - å®ç°å…¨é‡æ›´æ–°å’Œå¢é‡æ›´æ–°
å…¨é‡æ›´æ–°ï¼š1å°æ—¶è·å–ä¸€æ¬¡å…¨éƒ¨æ•°æ®
å¢é‡æ›´æ–°ï¼š1åˆ†é’Ÿæœç´¢æŒ‡å®šhashnameçš„æ•°æ®
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timedelta
from typing import List, Dict, Set, Optional
from dataclasses import asdict
import threading
import pickle

from integrated_price_system import PriceDiffItem, IntegratedPriceAnalyzer
from search_api_client import SearchManager, SearchResult
from analysis_manager import get_analysis_manager
from config import Config

logger = logging.getLogger(__name__)

class HashNameCache:
    """HashNameç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_file: str = "data/hashname_cache.pkl"):
        self.cache_file = cache_file
        self.hashnames: Set[str] = set()
        self.last_full_update = None
        self.load_cache()
    
    def save_cache(self):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        try:
            cache_data = {
                'hashnames': list(self.hashnames),
                'last_full_update': self.last_full_update.isoformat() if self.last_full_update else None
            }
            
            with open(self.cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
                
            logger.info(f"HashNameç¼“å­˜å·²ä¿å­˜: {len(self.hashnames)}ä¸ª")
            
        except Exception as e:
            logger.error(f"ä¿å­˜HashNameç¼“å­˜å¤±è´¥: {e}")
    
    def load_cache(self):
        """ä»æ–‡ä»¶åŠ è½½ç¼“å­˜"""
        try:
            with open(self.cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            self.hashnames = set(cache_data.get('hashnames', []))
            last_update_str = cache_data.get('last_full_update')
            
            if last_update_str:
                self.last_full_update = datetime.fromisoformat(last_update_str)
            
            logger.info(f"HashNameç¼“å­˜å·²åŠ è½½: {len(self.hashnames)}ä¸ª")
            
        except FileNotFoundError:
            logger.info("HashNameç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°ç¼“å­˜")
        except Exception as e:
            logger.error(f"åŠ è½½HashNameç¼“å­˜å¤±è´¥: {e}")
    
    def update_from_full_analysis(self, diff_items: List[PriceDiffItem]):
        """ä»å…¨é‡åˆ†æç»“æœæ›´æ–°ç¼“å­˜"""
        new_hashnames = set()
        
        for item in diff_items:
            # æå–hashnameï¼ˆä»nameæˆ–URLä¸­ï¼‰
            if hasattr(item, 'hash_name') and item.hash_name:
                new_hashnames.add(item.hash_name)
            elif item.name:
                # å¦‚æœæ²¡æœ‰hash_nameï¼Œä½¿ç”¨nameä½œä¸ºæœç´¢å…³é”®è¯
                new_hashnames.add(item.name)
        
        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(new_hashnames) > Config.INCREMENTAL_CACHE_SIZE:
            # æŒ‰æŸç§ä¼˜å…ˆçº§æ’åºï¼ˆä¾‹å¦‚ä»·å·®å¤§å°ï¼‰
            sorted_items = sorted(diff_items, key=lambda x: x.price_diff, reverse=True)
            new_hashnames = set()
            for item in sorted_items[:Config.INCREMENTAL_CACHE_SIZE]:
                if hasattr(item, 'hash_name') and item.hash_name:
                    new_hashnames.add(item.hash_name)
                elif item.name:
                    new_hashnames.add(item.name)
        
        self.hashnames = new_hashnames
        self.last_full_update = datetime.now()
        self.save_cache()
        
        logger.info(f"HashNameç¼“å­˜å·²æ›´æ–°: {len(self.hashnames)}ä¸ªå…³é”®è¯")
    
    def get_hashnames_for_search(self) -> List[str]:
        """è·å–ç”¨äºæœç´¢çš„hashnameåˆ—è¡¨"""
        return list(self.hashnames)
    
    def should_full_update(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å…¨é‡æ›´æ–°"""
        if not self.last_full_update:
            return True
        
        time_since_update = datetime.now() - self.last_full_update
        return time_since_update >= timedelta(hours=Config.FULL_UPDATE_INTERVAL_HOURS)

class UpdateManager:
    """æ›´æ–°ç®¡ç†å™¨ - åè°ƒå…¨é‡å’Œå¢é‡æ›´æ–°"""
    
    def __init__(self):
        self.is_running = False
        self.hashname_cache = HashNameCache()
        self.current_diff_items: List[PriceDiffItem] = []
        self.last_full_update = None
        self.last_incremental_update = None
        
        # çº¿ç¨‹æ§åˆ¶
        self.full_update_thread = None
        self.incremental_update_thread = None
        self.stop_event = threading.Event()
    
    def start(self):
        """å¯åŠ¨æ›´æ–°ç®¡ç†å™¨"""
        if self.is_running:
            logger.warning("æ›´æ–°ç®¡ç†å™¨å·²åœ¨è¿è¡Œ")
            return
        
        self.is_running = True
        self.stop_event.clear()
        
        logger.info("ğŸš€ å¯åŠ¨æ›´æ–°ç®¡ç†å™¨")
        
        # å¯åŠ¨å…¨é‡æ›´æ–°çº¿ç¨‹
        self.full_update_thread = threading.Thread(
            target=self._full_update_loop, 
            daemon=True, 
            name="FullUpdateLoop"
        )
        self.full_update_thread.start()
        
        # å¯åŠ¨å¢é‡æ›´æ–°çº¿ç¨‹
        self.incremental_update_thread = threading.Thread(
            target=self._incremental_update_loop, 
            daemon=True, 
            name="IncrementalUpdateLoop"
        )
        self.incremental_update_thread.start()
        
        # å¦‚æœéœ€è¦ï¼Œç«‹å³æ‰§è¡Œä¸€æ¬¡å…¨é‡æ›´æ–°
        if self.hashname_cache.should_full_update():
            logger.info("éœ€è¦å…¨é‡æ›´æ–°ï¼Œå¯åŠ¨åˆå§‹åŒ–æ›´æ–°...")
            self._trigger_full_update()
    
    def stop(self):
        """åœæ­¢æ›´æ–°ç®¡ç†å™¨"""
        if not self.is_running:
            return
        
        logger.info("ğŸ›‘ åœæ­¢æ›´æ–°ç®¡ç†å™¨")
        self.is_running = False
        self.stop_event.set()
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        if self.full_update_thread and self.full_update_thread.is_alive():
            self.full_update_thread.join(timeout=5)
        
        if self.incremental_update_thread and self.incremental_update_thread.is_alive():
            self.incremental_update_thread.join(timeout=5)
    
    def _full_update_loop(self):
        """å…¨é‡æ›´æ–°å¾ªç¯"""
        while self.is_running and not self.stop_event.is_set():
            try:
                if self.hashname_cache.should_full_update():
                    logger.info("â° å¼€å§‹å®šæ—¶å…¨é‡æ›´æ–°")
                    self._trigger_full_update()
                
                # ç­‰å¾…1å°æ—¶æˆ–ç›´åˆ°åœæ­¢
                if self.stop_event.wait(timeout=3600):  # 1å°æ—¶ = 3600ç§’
                    break
                    
            except Exception as e:
                logger.error(f"å…¨é‡æ›´æ–°å¾ªç¯å‡ºé”™: {e}")
                # å‡ºé”™åç­‰å¾…5åˆ†é’Ÿå†é‡è¯•
                if self.stop_event.wait(timeout=300):
                    break
    
    def _incremental_update_loop(self):
        """å¢é‡æ›´æ–°å¾ªç¯"""
        while self.is_running and not self.stop_event.is_set():
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰hashnameå¯ä»¥æœç´¢
                if self.hashname_cache.hashnames:
                    logger.info("ğŸ”„ å¼€å§‹å¢é‡æ›´æ–°")
                    self._trigger_incremental_update()
                else:
                    logger.debug("æ²¡æœ‰ç¼“å­˜çš„hashnameï¼Œè·³è¿‡å¢é‡æ›´æ–°")
                
                # ç­‰å¾…1åˆ†é’Ÿæˆ–ç›´åˆ°åœæ­¢
                if self.stop_event.wait(timeout=60):  # 1åˆ†é’Ÿ = 60ç§’
                    break
                    
            except Exception as e:
                logger.error(f"å¢é‡æ›´æ–°å¾ªç¯å‡ºé”™: {e}")
                # å‡ºé”™åç­‰å¾…30ç§’å†é‡è¯•
                if self.stop_event.wait(timeout=30):
                    break
    
    def _trigger_full_update(self):
        """è§¦å‘å…¨é‡æ›´æ–°"""
        manager = get_analysis_manager()
        analysis_id = f"full_update_{int(time.time())}"
        
        # å¯åŠ¨å…¨é‡åˆ†æ
        if not manager.start_analysis('full_update', analysis_id):
            logger.warning("å…¨é‡æ›´æ–°è·³è¿‡ï¼šå·²æœ‰åˆ†æåœ¨è¿è¡Œ")
            return
        
        def run_async_analysis():
            """åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥åˆ†æ"""
            try:
                # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥å…¨é‡åˆ†æ
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    diff_items = loop.run_until_complete(self._run_full_analysis())
                    
                    if diff_items:
                        # æ›´æ–°å½“å‰æ•°æ®
                        self.current_diff_items = diff_items
                        self.last_full_update = datetime.now()
                        
                        # æ›´æ–°hashnameç¼“å­˜
                        self.hashname_cache.update_from_full_analysis(diff_items)
                        
                        # å®Œæˆåˆ†æ
                        manager.finish_analysis(analysis_id, diff_items)
                        
                        logger.info(f"âœ… å…¨é‡æ›´æ–°å®Œæˆ: {len(diff_items)}ä¸ªå•†å“")
                    else:
                        logger.warning("å…¨é‡æ›´æ–°æœªè·å–åˆ°æ•°æ®")
                        manager.finish_analysis(analysis_id, [])
                        
                finally:
                    loop.close()
                    
            except Exception as e:
                logger.error(f"å…¨é‡æ›´æ–°å¤±è´¥: {e}")
                manager.finish_analysis(analysis_id)
        
        # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        thread = threading.Thread(target=run_async_analysis, daemon=True)
        thread.start()
    
    def _trigger_incremental_update(self):
        """è§¦å‘å¢é‡æ›´æ–°"""
        manager = get_analysis_manager()
        analysis_id = f"incremental_update_{int(time.time())}"
        
        # å°è¯•å¯åŠ¨å¢é‡åˆ†æï¼ˆéé˜»å¡ï¼‰
        if not manager.start_analysis('incremental_update', analysis_id, force=False):
            logger.debug("å¢é‡æ›´æ–°è·³è¿‡ï¼šå·²æœ‰åˆ†æåœ¨è¿è¡Œ")
            return
        
        def run_async_analysis():
            """åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥åˆ†æ"""
            try:
                # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥å¢é‡åˆ†æ
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    incremental_items = loop.run_until_complete(self._run_incremental_analysis())
                    
                    if incremental_items:
                        # åˆå¹¶åˆ°å½“å‰æ•°æ®ä¸­ï¼ˆå»é‡ï¼‰
                        self._merge_incremental_data(incremental_items)
                        
                        # æ›´æ–°å…¨å±€ç¼“å­˜
                        manager.finish_analysis(analysis_id, self.current_diff_items)
                        
                        logger.info(f"âœ… å¢é‡æ›´æ–°å®Œæˆ: æ–°å¢/æ›´æ–° {len(incremental_items)}ä¸ªå•†å“")
                    else:
                        logger.debug("å¢é‡æ›´æ–°æ— æ–°æ•°æ®")
                        manager.finish_analysis(analysis_id, [])
                    
                    self.last_incremental_update = datetime.now()
                        
                finally:
                    loop.close()
                    
            except Exception as e:
                logger.error(f"å¢é‡æ›´æ–°å¤±è´¥: {e}")
                manager.finish_analysis(analysis_id)
        
        # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        thread = threading.Thread(target=run_async_analysis, daemon=True)
        thread.start()
    
    async def _run_full_analysis(self) -> List[PriceDiffItem]:
        """è¿è¡Œå…¨é‡åˆ†æ"""
        async with IntegratedPriceAnalyzer() as analyzer:
            return await analyzer.analyze_price_differences(
                max_output_items=Config.MAX_OUTPUT_ITEMS
            )
    
    async def _run_incremental_analysis(self) -> List[PriceDiffItem]:
        """è¿è¡Œå¢é‡åˆ†æ"""
        hashnames = self.hashname_cache.get_hashnames_for_search()
        if not hashnames:
            return []
        
        incremental_items = []
        
        async with SearchManager() as search_manager:
            # é€ä¸ªæœç´¢hashnameï¼ˆé™åˆ¶å¹¶å‘ï¼‰
            semaphore = asyncio.Semaphore(5)  # æœ€å¤š5ä¸ªå¹¶å‘æœç´¢
            
            async def search_and_analyze(keyword):
                async with semaphore:
                    try:
                        # æœç´¢ä¸¤ä¸ªå¹³å°
                        results = await search_manager.search_both_platforms(keyword)
                        
                        # åˆ†æä»·å·®
                        diff_items = self._analyze_search_results(results)
                        return diff_items
                        
                    except Exception as e:
                        logger.error(f"å¢é‡æœç´¢å¤±è´¥ {keyword}: {e}")
                        return []
            
            # æ‰¹é‡å¤„ç†ï¼ˆé¿å…è¿‡å¤šå¹¶å‘ï¼‰
            batch_size = 10
            for i in range(0, len(hashnames), batch_size):
                batch_keywords = hashnames[i:i + batch_size]
                
                tasks = [search_and_analyze(keyword) for keyword in batch_keywords]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in batch_results:
                    if isinstance(result, Exception):
                        logger.error(f"æ‰¹é‡æœç´¢å¼‚å¸¸: {result}")
                    elif result:
                        incremental_items.extend(result)
                
                # æ‰¹æ¬¡é—´å»¶è¿Ÿ
                await asyncio.sleep(1)
        
        return incremental_items
    
    def _analyze_search_results(self, search_results: Dict) -> List[PriceDiffItem]:
        """åˆ†ææœç´¢ç»“æœï¼Œè®¡ç®—ä»·å·®"""
        diff_items = []
        
        youpin_results = search_results.get('youpin', [])
        buff_results = search_results.get('buff', [])
        
        # æŒ‰hash_nameåŒ¹é…ä»·æ ¼
        youpin_prices = {item.hash_name: item for item in youpin_results if item.hash_name}
        buff_prices = {item.hash_name: item for item in buff_results if item.hash_name}
        
        # å¦‚æœæ²¡æœ‰hash_nameï¼Œä½¿ç”¨nameåŒ¹é…
        if not youpin_prices:
            youpin_prices = {item.name: item for item in youpin_results}
        if not buff_prices:
            buff_prices = {item.name: item for item in buff_results}
        
        # åŒ¹é…å¹¶è®¡ç®—ä»·å·®
        for hash_name, buff_item in buff_prices.items():
            youpin_item = youpin_prices.get(hash_name)
            
            if youpin_item and buff_item.price > 0 and youpin_item.price > 0:
                # æ£€æŸ¥Buffä»·æ ¼æ˜¯å¦åœ¨ç­›é€‰èŒƒå›´å†…
                if not Config.is_buff_price_in_range(buff_item.price):
                    continue
                
                price_diff = youpin_item.price - buff_item.price
                
                # æ£€æŸ¥ä»·å·®æ˜¯å¦ç¬¦åˆè¦æ±‚
                if Config.is_price_diff_in_range(price_diff):
                    profit_rate = (price_diff / buff_item.price) * 100 if buff_item.price > 0 else 0
                    
                    diff_item = PriceDiffItem(
                        id=buff_item.id,
                        name=buff_item.name,
                        buff_price=buff_item.price,
                        youpin_price=youpin_item.price,
                        price_diff=price_diff,
                        profit_rate=profit_rate,
                        buff_url=buff_item.market_url,
                        youpin_url=youpin_item.market_url,
                        image_url=buff_item.image_url,
                        category="æœç´¢ç»“æœ",
                        last_updated=datetime.now()
                    )
                    
                    diff_items.append(diff_item)
        
        return diff_items
    
    def _merge_incremental_data(self, incremental_items: List[PriceDiffItem]):
        """åˆå¹¶å¢é‡æ•°æ®åˆ°å½“å‰æ•°æ®ä¸­"""
        # åˆ›å»ºå½“å‰æ•°æ®çš„ç´¢å¼•ï¼ˆæŒ‰nameæˆ–idï¼‰
        current_index = {}
        for i, item in enumerate(self.current_diff_items):
            key = f"{item.name}_{item.id}" if item.id else item.name
            current_index[key] = i
        
        # åˆå¹¶æ–°æ•°æ®
        for new_item in incremental_items:
            key = f"{new_item.name}_{new_item.id}" if new_item.id else new_item.name
            
            if key in current_index:
                # æ›´æ–°ç°æœ‰å•†å“
                self.current_diff_items[current_index[key]] = new_item
            else:
                # æ·»åŠ æ–°å•†å“
                self.current_diff_items.append(new_item)
        
        # æŒ‰ä»·å·®æ’åº
        self.current_diff_items.sort(key=lambda x: x.price_diff, reverse=True)
        
        # é™åˆ¶æ•°é‡
        if len(self.current_diff_items) > Config.MAX_OUTPUT_ITEMS:
            self.current_diff_items = self.current_diff_items[:Config.MAX_OUTPUT_ITEMS]
    
    def get_current_data(self) -> List[PriceDiffItem]:
        """è·å–å½“å‰æ•°æ®"""
        return self.current_diff_items.copy()
    
    def get_status(self) -> Dict:
        """è·å–æ›´æ–°çŠ¶æ€"""
        return {
            'is_running': self.is_running,
            'last_full_update': self.last_full_update.isoformat() if self.last_full_update else None,
            'last_incremental_update': self.last_incremental_update.isoformat() if self.last_incremental_update else None,
            'current_items_count': len(self.current_diff_items),
            'cached_hashnames_count': len(self.hashname_cache.hashnames),
            'should_full_update': self.hashname_cache.should_full_update(),
            'full_update_interval_hours': Config.FULL_UPDATE_INTERVAL_HOURS,
            'incremental_update_interval_minutes': Config.INCREMENTAL_UPDATE_INTERVAL_MINUTES
        }
    
    def force_full_update(self):
        """å¼ºåˆ¶å…¨é‡æ›´æ–°"""
        logger.info("ğŸ”„ å¼ºåˆ¶æ‰§è¡Œå…¨é‡æ›´æ–°")
        threading.Thread(target=self._trigger_full_update, daemon=True).start()
    
    def force_incremental_update(self):
        """å¼ºåˆ¶å¢é‡æ›´æ–°"""
        logger.info("ğŸ”„ å¼ºåˆ¶æ‰§è¡Œå¢é‡æ›´æ–°")
        threading.Thread(target=self._trigger_incremental_update, daemon=True).start()

# å…¨å±€æ›´æ–°ç®¡ç†å™¨å®ä¾‹
_update_manager_instance = None

def get_update_manager() -> UpdateManager:
    """è·å–æ›´æ–°ç®¡ç†å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _update_manager_instance
    if _update_manager_instance is None:
        _update_manager_instance = UpdateManager()
    return _update_manager_instance

# æµ‹è¯•åŠŸèƒ½
async def test_update_manager():
    """æµ‹è¯•æ›´æ–°ç®¡ç†å™¨"""
    print("ğŸ§ª æµ‹è¯•æ›´æ–°ç®¡ç†å™¨")
    print("="*50)
    
    manager = get_update_manager()
    
    # æ˜¾ç¤ºçŠ¶æ€
    status = manager.get_status()
    print(f"çŠ¶æ€: {status}")
    
    # å¼ºåˆ¶æ‰§è¡Œä¸€æ¬¡å…¨é‡æ›´æ–°
    print("\nğŸ”„ æ‰§è¡Œå…¨é‡æ›´æ–°æµ‹è¯•...")
    manager.force_full_update()
    
    # ç­‰å¾…ä¸€æ®µæ—¶é—´
    await asyncio.sleep(30)
    
    # æ˜¾ç¤ºç»“æœ
    data = manager.get_current_data()
    print(f"å½“å‰æ•°æ®: {len(data)}ä¸ªå•†å“")
    
    if data:
        print("å‰3ä¸ªå•†å“:")
        for i, item in enumerate(data[:3], 1):
            print(f"  {i}. {item.name}: ä»·å·®Â¥{item.price_diff:.2f}")

if __name__ == "__main__":
    asyncio.run(test_update_manager()) 