#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ›´æ–°ç®¡ç†å™¨ - å®ç°å…¨é‡æ›´æ–°å’Œå¢é‡æ›´æ–°
å…¨é‡æ›´æ–°ï¼š1å°æ—¶è·å–ä¸€æ¬¡å…¨éƒ¨æ•°æ®
å¢é‡æ›´æ–°ï¼š1åˆ†é’Ÿæœç´¢æŒ‡å®šhashnameçš„æ•°æ®
"""

import asyncio
import json
import logging
import os
import time
from datetime import datetime, timedelta
from typing import List, Dict, Set, Optional
from dataclasses import asdict
import threading
import pickle

from integrated_price_system import PriceDiffItem, IntegratedPriceAnalyzer
from search_api_client import SearchManager, SearchResult
from analysis_manager import get_analysis_manager
from config import Config
from data_storage import DataStorage
from asyncio_utils import SafeEventLoop, safe_close_loop

# ğŸ”¥ ä½¿ç”¨å¢å¼ºçš„æ—¥å¿—é…ç½®
try:
    from log_config import setup_logging
    logger = setup_logging(log_level='INFO', app_name='update_manager')
except ImportError:
    logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HashNameCache:
    """HashNameç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_file: str = "data/hashname_cache.pkl"):
        self.cache_file = cache_file
        # ğŸ”¥ ä¿®æ”¹ï¼šå­˜å‚¨hash_name -> åˆ©æ¶¦ç‡çš„æ˜ å°„
        self.hashname_profits: Dict[str, float] = {}
        self.last_full_update = None
        self.load_cache()
    
    def save_cache(self):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        try:
            cache_data = {
                'hashname_profits': self.hashname_profits,
                'last_full_update': self.last_full_update.isoformat() if self.last_full_update else None
            }
            
            with open(self.cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
                
            logger.info(f"HashNameç¼“å­˜å·²ä¿å­˜: {len(self.hashname_profits)}ä¸ªï¼ˆå«åˆ©æ¶¦ç‡ä¿¡æ¯ï¼‰")
            
        except Exception as e:
            logger.error(f"ä¿å­˜HashNameç¼“å­˜å¤±è´¥: {e}")
    
    def load_cache(self):
        """ä»æ–‡ä»¶åŠ è½½ç¼“å­˜"""
        try:
            with open(self.cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            # ğŸ”¥ å…¼å®¹æ—§æ ¼å¼å’Œæ–°æ ¼å¼
            if 'hashname_profits' in cache_data:
                # æ–°æ ¼å¼ï¼šåŒ…å«åˆ©æ¶¦ç‡ä¿¡æ¯
                self.hashname_profits = cache_data.get('hashname_profits', {})
                logger.info(f"HashNameç¼“å­˜å·²åŠ è½½: {len(self.hashname_profits)}ä¸ªï¼ˆå«åˆ©æ¶¦ç‡ä¿¡æ¯ï¼‰")
            else:
                # æ—§æ ¼å¼ï¼šåªæœ‰hashnamesåˆ—è¡¨ï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼
                old_hashnames = cache_data.get('hashnames', [])
                self.hashname_profits = {name: 0.0 for name in old_hashnames}
                logger.info(f"HashNameç¼“å­˜å·²åŠ è½½ï¼ˆæ—§æ ¼å¼è½¬æ¢ï¼‰: {len(self.hashname_profits)}ä¸ª")
            
            last_update_str = cache_data.get('last_full_update')
            if last_update_str:
                self.last_full_update = datetime.fromisoformat(last_update_str)
            
        except FileNotFoundError:
            logger.info("HashNameç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°ç¼“å­˜")
        except Exception as e:
            logger.error(f"åŠ è½½HashNameç¼“å­˜å¤±è´¥: {e}")
    
    @property
    def hashnames(self) -> Set[str]:
        """å‘åå…¼å®¹ï¼šè¿”å›æ‰€æœ‰hash_nameçš„é›†åˆ"""
        return set(self.hashname_profits.keys())
    
    def update_from_full_analysis(self, diff_items: List[PriceDiffItem]):
        """ä»å…¨é‡åˆ†æç»“æœæ›´æ–°ç¼“å­˜"""
        new_hashname_profits = {}
        
        for item in diff_items:
            # ğŸ”¥ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†ä¸¤ç§ä¸åŒçš„PriceDiffItemç±»å‹
            hash_name = None
            profit_rate = 0.0
            
            # æ–¹å¼1: integrated_price_system.pyçš„PriceDiffItem (ç›´æ¥hash_nameå±æ€§)
            if hasattr(item, 'hash_name') and item.hash_name:
                hash_name = item.hash_name
                profit_rate = getattr(item, 'profit_rate', 0.0)
                logger.debug(f"   ç¼“å­˜hash_name (ç›´æ¥): {hash_name}, åˆ©æ¶¦ç‡: {profit_rate:.2%}")
            
            # æ–¹å¼2: models.pyçš„PriceDiffItem (é€šè¿‡skin_item.hash_name)
            elif hasattr(item, 'skin_item') and hasattr(item.skin_item, 'hash_name') and item.skin_item.hash_name:
                hash_name = item.skin_item.hash_name
                profit_rate = getattr(item, 'profit_rate', 0.0)
                logger.debug(f"   ç¼“å­˜hash_name (skin_item): {hash_name}, åˆ©æ¶¦ç‡: {profit_rate:.2%}")
            
            # å¦‚æœæ‰¾åˆ°äº†æœ‰æ•ˆçš„hash_nameï¼Œä½¿ç”¨å®ƒ
            if hash_name:
                new_hashname_profits[hash_name] = profit_rate
            else:
                # å¤‡é€‰ï¼šå¦‚æœæ²¡æœ‰hash_nameï¼Œä½¿ç”¨nameï¼ˆä½†ä¼šå½±å“æœç´¢æ•ˆæœï¼‰
                item_name = getattr(item, 'name', None) or (getattr(item, 'skin_item', None) and getattr(item.skin_item, 'name', None))
                if item_name:
                    new_hashname_profits[item_name] = profit_rate
                    logger.warning(f"   âš ï¸ ä½¿ç”¨nameä½œä¸ºç¼“å­˜å…³é”®è¯: {item_name}, åˆ©æ¶¦ç‡: {profit_rate:.2%}")
        
        # é™åˆ¶ç¼“å­˜å¤§å°ï¼Œä¿ç•™åˆ©æ¶¦ç‡æœ€é«˜çš„å•†å“
        if len(new_hashname_profits) > Config.INCREMENTAL_CACHE_SIZE:
            # æŒ‰åˆ©æ¶¦ç‡æ’åºï¼Œä¿ç•™å‰Nä¸ª
            sorted_items = sorted(new_hashname_profits.items(), key=lambda x: x[1], reverse=True)
            new_hashname_profits = dict(sorted_items[:Config.INCREMENTAL_CACHE_SIZE])
            logger.info(f"   é™åˆ¶ç¼“å­˜å¤§å°åˆ° {Config.INCREMENTAL_CACHE_SIZE}ä¸ªï¼Œä¿ç•™åˆ©æ¶¦ç‡æœ€é«˜çš„å•†å“")
        
        self.hashname_profits = new_hashname_profits
        self.last_full_update = datetime.now()
        self.save_cache()
        
        # ğŸ”¥ ç»Ÿè®¡ä¿¡æ¯
        if new_hashname_profits:
            max_profit = max(new_hashname_profits.values())
            min_profit = min(new_hashname_profits.values())
            avg_profit = sum(new_hashname_profits.values()) / len(new_hashname_profits)
            
            logger.info(f"HashNameç¼“å­˜å·²æ›´æ–°: {len(new_hashname_profits)}ä¸ªå…³é”®è¯")
            logger.info(f"   ğŸ“ˆ åˆ©æ¶¦ç‡èŒƒå›´: {min_profit:.2%} ~ {max_profit:.2%}")
            logger.info(f"   ğŸ“Š å¹³å‡åˆ©æ¶¦ç‡: {avg_profit:.2%}")
        else:
            logger.warning("HashNameç¼“å­˜æ›´æ–°åä¸ºç©º")
    
    def get_hashnames_for_search(self) -> List[str]:
        """è·å–ç”¨äºæœç´¢çš„hashnameåˆ—è¡¨ï¼ŒåŒæ—¶è¿”å›åˆ©æ¶¦ç‡å‰25å’Œå·®ä»·å‰25çš„å•†å“"""
        if not self.hashname_profits:
            logger.warning("æ²¡æœ‰ç¼“å­˜çš„hashnameå¯ä¾›æœç´¢")
            return []
        
        # ğŸ”¥ ä¿®æ”¹ï¼šåŒæ—¶è·å–åˆ©æ¶¦ç‡å‰25å’Œå·®ä»·å‰25çš„å•†å“
        # 1. æŒ‰åˆ©æ¶¦ç‡æ’åº
        profit_sorted = sorted(self.hashname_profits.items(), key=lambda x: x[1], reverse=True)
        top_profit = profit_sorted[:25]
        
        # 2. æŒ‰å·®ä»·æ’åºï¼ˆä»å…¨é‡æ•°æ®ä¸­è·å–ï¼‰
        try:
            from saved_data_processor import get_saved_data_processor
            processor = get_saved_data_processor()
            if processor.has_valid_full_data():
                diff_items, _ = processor.reprocess_with_current_filters()
                if diff_items:
                    # åˆ›å»ºhash_nameåˆ°å·®ä»·çš„æ˜ å°„
                    price_diff_map = {}
                    for item in diff_items:
                        hash_name = getattr(item, 'hash_name', None) or (getattr(item, 'skin_item', None) and getattr(item.skin_item, 'hash_name', None))
                        if hash_name:
                            price_diff_map[hash_name] = getattr(item, 'price_diff', 0.0)
                    
                    # æŒ‰å·®ä»·æ’åº
                    diff_sorted = sorted(price_diff_map.items(), key=lambda x: x[1], reverse=True)
                    top_diff = diff_sorted[:25]
                else:
                    top_diff = []
            else:
                top_diff = []
        except Exception as e:
            logger.error(f"è·å–å·®ä»·æ’åºå¤±è´¥: {e}")
            top_diff = []
        
        # åˆå¹¶ä¸¤ä¸ªåˆ—è¡¨ï¼Œå»é‡
        all_hashnames = set()
        for hash_name, _ in top_profit:
            all_hashnames.add(hash_name)
        for hash_name, _ in top_diff:
            all_hashnames.add(hash_name)
        
        hashnames_list = list(all_hashnames)
        
        logger.info(f"ğŸ¯ å¢é‡æœç´¢å…³é”®è¯ï¼ˆåˆ©æ¶¦ç‡å‰25 + å·®ä»·å‰25ï¼‰:")
        logger.info(f"   ğŸ“Š ä»{len(self.hashname_profits)}ä¸ªä¸­é€‰æ‹©{len(hashnames_list)}ä¸ª")
        
        if top_profit:
            logger.info(f"   ğŸ“ˆ åˆ©æ¶¦ç‡èŒƒå›´: {top_profit[-1][1]:.2%} ~ {top_profit[0][1]:.2%}")
            
            # æ˜¾ç¤ºå‰5ä¸ªå•†å“
            logger.info(f"   ğŸ” åˆ©æ¶¦ç‡å‰5ä¸ªå•†å“:")
            for i, (hash_name, profit_rate) in enumerate(top_profit[:5]):
                logger.info(f"      {i+1}. {hash_name} ({profit_rate:.2%})")
        
        if top_diff:
            logger.info(f"   ğŸ’° å·®ä»·å‰5ä¸ªå•†å“:")
            for i, (hash_name, price_diff) in enumerate(top_diff[:5]):
                logger.info(f"      {i+1}. {hash_name} (å·®ä»·: Â¥{price_diff:.2f})")
        
        return hashnames_list
    
    def should_full_update(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å…¨é‡æ›´æ–°"""
        if not self.last_full_update:
            return True
        
        time_since_update = datetime.now() - self.last_full_update
        return time_since_update >= timedelta(hours=Config.FULL_UPDATE_INTERVAL_HOURS)

class UpdateManager:
    """æ›´æ–°ç®¡ç†å™¨ - åè°ƒå…¨é‡å’Œå¢é‡æ›´æ–°"""
    
    def __init__(self):
        self.is_running = False
        self.hashname_cache = HashNameCache()
        self.current_diff_items: List[PriceDiffItem] = []
        self.last_full_update = None
        self.last_incremental_update = None
        
        # ğŸ”¥ æ–°å¢ï¼šæ•°æ®å­˜å‚¨ç®¡ç†å™¨
        try:
            from data_storage import DataStorage
            self.data_storage = DataStorage()
        except ImportError:
            self.data_storage = None
        
        # ğŸ”¥ æ–°å¢ï¼šåˆå§‹å…¨é‡æ›´æ–°å®Œæˆæ ‡å¿—
        self.initial_full_update_completed = False
        
        # çº¿ç¨‹æ§åˆ¶
        self.full_update_thread = None
        self.incremental_update_thread = None
        self.stop_event = threading.Event()
    
    def start(self):
        """å¯åŠ¨æ›´æ–°ç®¡ç†å™¨"""
        if self.is_running:
            logger.warning("æ›´æ–°ç®¡ç†å™¨å·²åœ¨è¿è¡Œ")
            return
        
        self.is_running = True
        self.stop_event.clear()
        
        logger.info("ğŸš€ å¯åŠ¨æ›´æ–°ç®¡ç†å™¨")
        
        # ğŸ”¥ æ–°å¢ï¼šä¼˜å…ˆæ£€æŸ¥full dataæ–‡ä»¶å¹¶é‡æ–°ç”Ÿæˆç¼“å­˜
        if self._regenerate_cache_from_full_data():
            logger.info("âœ… ä»full dataæ–‡ä»¶é‡æ–°ç”Ÿæˆç¼“å­˜æˆåŠŸ")
            self.initial_full_update_completed = True
        else:
            # ğŸ”¥ åŸæœ‰é€»è¾‘ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹å…¨é‡æ›´æ–°
            needs_initial_update = self.hashname_cache.should_full_update()
            if needs_initial_update:
                logger.info("ğŸ“Š éœ€è¦åˆå§‹å…¨é‡æ›´æ–°ï¼Œå°†ç­‰å¾…å®Œæˆåå†å¯åŠ¨å¢é‡æ›´æ–°")
                self.initial_full_update_completed = False
            else:
                logger.info("ğŸ“Š æœ‰ç¼“å­˜æ•°æ®ï¼Œå¯ç›´æ¥å¯åŠ¨å¢é‡æ›´æ–°")
                self.initial_full_update_completed = True
                    
                # ğŸ”¥ å°è¯•åŠ è½½å·²æœ‰çš„ä»·å·®æ•°æ®
                try:
                    self._load_latest_data()
                    if self.current_diff_items:
                        logger.info(f"âœ… æˆåŠŸåŠ è½½ç¼“å­˜æ•°æ®: {len(self.current_diff_items)}ä¸ªå•†å“")
                    else:
                        logger.warning("âš ï¸ ç¼“å­˜æ•°æ®ä¸ºç©ºï¼Œå°†å¼ºåˆ¶æ‰§è¡Œå…¨é‡æ›´æ–°")
                        self.initial_full_update_completed = False
                        self.hashname_cache.hashname_profits.clear()
                        self.hashname_cache.last_full_update = None
                except Exception as e:
                    logger.error(f"âŒ åŠ è½½ç¼“å­˜æ•°æ®å¤±è´¥: {e}ï¼Œå°†å¼ºåˆ¶æ‰§è¡Œå…¨é‡æ›´æ–°")
                    self.initial_full_update_completed = False
                    self.hashname_cache.hashname_profits.clear()
                    self.hashname_cache.last_full_update = None
        
        # å¯åŠ¨å…¨é‡æ›´æ–°çº¿ç¨‹
        self.full_update_thread = threading.Thread(
            target=self._full_update_loop, 
            daemon=True, 
            name="FullUpdateLoop"
        )
        self.full_update_thread.start()
        
        # å¯åŠ¨å¢é‡æ›´æ–°çº¿ç¨‹
        self.incremental_update_thread = threading.Thread(
            target=self._incremental_update_loop, 
            daemon=True, 
            name="IncrementalUpdateLoop"
        )
        self.incremental_update_thread.start()
        
        logger.info("ğŸ¯ å¯åŠ¨å®Œæˆï¼Œå®šæ—¶å¾ªç¯å°†å¤„ç†æ›´æ–°ä»»åŠ¡")
    
    def stop(self):
        """åœæ­¢æ›´æ–°ç®¡ç†å™¨"""
        if not self.is_running:
            return
        
        logger.info("ğŸ›‘ åœæ­¢æ›´æ–°ç®¡ç†å™¨")
        self.is_running = False
        self.stop_event.set()
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        if self.full_update_thread and self.full_update_thread.is_alive():
            self.full_update_thread.join(timeout=5)
        
        if self.incremental_update_thread and self.incremental_update_thread.is_alive():
            self.incremental_update_thread.join(timeout=5)
    
    def _full_update_loop(self):
        """å…¨é‡æ›´æ–°å¾ªç¯"""
        # ğŸ”¥ ä¿®å¤æ­»é”ï¼šå¦‚æœéœ€è¦åˆå§‹æ›´æ–°ï¼Œç«‹å³æ‰§è¡Œä¸€æ¬¡
        if not self.initial_full_update_completed and self.hashname_cache.should_full_update():
            logger.info("ğŸ”„ ç«‹å³æ‰§è¡Œåˆå§‹å…¨é‡æ›´æ–°...")
            self._trigger_full_update(is_initial=True)
            
            # ç­‰å¾…åˆå§‹æ›´æ–°å®Œæˆ
            logger.info("â³ ç­‰å¾…åˆå§‹å…¨é‡æ›´æ–°å®Œæˆ...")
            while self.is_running and not self.stop_event.is_set():
                if self.initial_full_update_completed:
                    break
                # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                if self.stop_event.wait(timeout=5):
                    return
        
        logger.info("âœ… å¼€å§‹å…¨é‡æ›´æ–°å®šæ—¶å¾ªç¯")
        
        while self.is_running and not self.stop_event.is_set():
            try:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å®šæ—¶å…¨é‡æ›´æ–°
                if self.hashname_cache.should_full_update():
                    logger.info("â° å¼€å§‹å®šæ—¶å…¨é‡æ›´æ–°")
                    self._trigger_full_update()
                
                # ç­‰å¾…1å°æ—¶æˆ–ç›´åˆ°åœæ­¢
                if self.stop_event.wait(timeout=3600):  # 1å°æ—¶ = 3600ç§’
                    break
                    
            except Exception as e:
                logger.error(f"å…¨é‡æ›´æ–°å¾ªç¯å‡ºé”™: {e}")
                # å‡ºé”™åç­‰å¾…5åˆ†é’Ÿå†é‡è¯•
                if self.stop_event.wait(timeout=300):
                    break
    
    def _incremental_update_loop(self):
        """å¢é‡æ›´æ–°å¾ªç¯"""
        # ğŸ”¥ æ–°å¢ï¼šç­‰å¾…åˆå§‹å…¨é‡æ›´æ–°å®Œæˆ
        if not self.initial_full_update_completed:
            logger.info("â³ å¢é‡æ›´æ–°ç­‰å¾…åˆå§‹å…¨é‡æ›´æ–°å®Œæˆ...")
            while self.is_running and not self.stop_event.is_set():
                if self.initial_full_update_completed:
                    logger.info("âœ… åˆå§‹å…¨é‡æ›´æ–°å·²å®Œæˆï¼Œå¼€å§‹å¢é‡æ›´æ–°å¾ªç¯")
                    break
                # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                if self.stop_event.wait(timeout=5):
                    return
        
        # ğŸ”¥ å¼€å§‹æ­£å¸¸çš„å¢é‡æ›´æ–°å¾ªç¯
        while self.is_running and not self.stop_event.is_set():
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰hashnameå¯ä»¥æœç´¢
                if self.hashname_cache.hashnames:
                    logger.info("ğŸ”„ å¼€å§‹å¢é‡æ›´æ–°")
                    self._trigger_incremental_update()
                else:
                    logger.debug("æ²¡æœ‰ç¼“å­˜çš„hashnameï¼Œè·³è¿‡å¢é‡æ›´æ–°")
                
                # ä½¿ç”¨é…ç½®çš„å¢é‡æ›´æ–°é—´éš”
                interval_seconds = Config.INCREMENTAL_UPDATE_INTERVAL_MINUTES * 60
                if self.stop_event.wait(timeout=interval_seconds):
                    break
                    
            except Exception as e:
                logger.error(f"å¢é‡æ›´æ–°å¾ªç¯å‡ºé”™: {e}")
                # å‡ºé”™åç­‰å¾…30ç§’å†é‡è¯•
                if self.stop_event.wait(timeout=30):
                    break
    
    def _trigger_full_update(self, is_initial=False):
        """è§¦å‘å…¨é‡æ›´æ–°"""
        manager = get_analysis_manager()
        analysis_id = f"full_update_{int(time.time())}"
        
        # å°è¯•å¯åŠ¨å…¨é‡åˆ†æï¼ˆå¼ºåˆ¶æ¨¡å¼ï¼‰
        if not manager.start_analysis('full_update', analysis_id, force=True):
            logger.warning("å…¨é‡æ›´æ–°è·³è¿‡ï¼šæ— æ³•å¯åŠ¨åˆ†æ")
            return
        
        def run_async_analysis():
            """åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥åˆ†æ"""
            try:
                # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥å…¨é‡åˆ†æ
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    # è¿è¡Œå…¨é‡åˆ†æ
                    updated_items = loop.run_until_complete(self._run_full_analysis())
                    
                    if updated_items:
                        # æ›´æ–°å½“å‰æ•°æ®
                        self.current_diff_items = updated_items
                        
                        # ä¿å­˜å½“å‰æ•°æ®åˆ°æ–‡ä»¶
                        self._save_current_data()
                        
                        # æ›´æ–°å…¨å±€ç¼“å­˜
                        manager.finish_analysis(analysis_id, updated_items)
                        
                        logger.info(f"âœ… å…¨é‡æ›´æ–°å®Œæˆ: åˆ†æå‡º {len(updated_items)} ä¸ªç¬¦åˆæ¡ä»¶çš„å•†å“")
                    else:
                        logger.info("ğŸ“­ å…¨é‡æ›´æ–°æœªå‘ç°ç¬¦åˆæ¡ä»¶çš„å•†å“")
                        manager.finish_analysis(analysis_id, [])
                    
                    self.last_full_update = datetime.now()
                    
                    # å¦‚æœæ˜¯åˆå§‹å…¨é‡æ›´æ–°ï¼Œæ ‡è®°ä¸ºå®Œæˆ
                    if is_initial:
                        self.initial_full_update_completed = True
                        logger.info("âœ… åˆå§‹å…¨é‡æ›´æ–°å®Œæˆï¼Œå¯ä»¥å¼€å§‹å¢é‡æ›´æ–°")
                        
                finally:
                    # ç¡®ä¿æ‰€æœ‰å¼‚æ­¥èµ„æºéƒ½è¢«æ¸…ç†
                    try:
                        # ç­‰å¾…æ‰€æœ‰æŒ‚èµ·çš„ä»»åŠ¡å®Œæˆ
                        pending = asyncio.all_tasks(loop)
                        if pending:
                            logger.debug(f"ç­‰å¾… {len(pending)} ä¸ªæŒ‚èµ·çš„ä»»åŠ¡å®Œæˆ...")
                            loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
                    except Exception as e:
                        logger.debug(f"æ¸…ç†æŒ‚èµ·ä»»åŠ¡æ—¶å‡ºé”™: {e}")
                    finally:
                        loop.close()
                    
            except Exception as e:
                logger.error(f"å…¨é‡æ›´æ–°å¤±è´¥: {e}")
                manager.finish_analysis(analysis_id)
        
        # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        thread = threading.Thread(target=run_async_analysis, daemon=True)
        thread.start()
    
    def _trigger_incremental_update(self):
        """è§¦å‘å¢é‡æ›´æ–°"""
        manager = get_analysis_manager()
        analysis_id = f"incremental_update_{int(time.time())}"
        
        # å°è¯•å¯åŠ¨å¢é‡åˆ†æï¼ˆéé˜»å¡ï¼‰
        if not manager.start_analysis('incremental_update', analysis_id, force=False):
            logger.debug("å¢é‡æ›´æ–°è·³è¿‡ï¼šå·²æœ‰åˆ†æåœ¨è¿è¡Œ")
            return
        
        def run_async_analysis():
            """åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥åˆ†æ"""
            try:
                # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥å¢é‡åˆ†æ
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    # ğŸ”¥ æ–°çš„å¢é‡æ›´æ–°é€»è¾‘ï¼šæœç´¢->æ›´æ–°æ–‡ä»¶->é‡æ–°åˆ†æ
                    updated_items = loop.run_until_complete(self._run_incremental_analysis())
                    
                    if updated_items:
                        # ğŸ”¥ ç›´æ¥ä½¿ç”¨é‡æ–°åˆ†æçš„ç»“æœï¼Œä¸éœ€è¦åˆå¹¶
                        self.current_diff_items = updated_items
                        
                        # ä¿å­˜å½“å‰æ•°æ®åˆ°æ–‡ä»¶
                        self._save_current_data()
                        
                        # æ›´æ–°å…¨å±€ç¼“å­˜
                        manager.finish_analysis(analysis_id, updated_items)
                        
                        logger.info(f"âœ… å¢é‡æ›´æ–°å®Œæˆ: åŸºäºæœ€æ–°æ•°æ®åˆ†æå‡º {len(updated_items)} ä¸ªç¬¦åˆæ¡ä»¶çš„å•†å“")
                    else:
                        logger.info("ğŸ“­ å¢é‡æ›´æ–°æœªå‘ç°ç¬¦åˆæ¡ä»¶çš„å•†å“")
                        manager.finish_analysis(analysis_id, [])
                    
                    self.last_incremental_update = datetime.now()
                        
                finally:
                    # ğŸ”¥ ä¼˜åŒ–ï¼šç¡®ä¿æ‰€æœ‰å¼‚æ­¥èµ„æºéƒ½è¢«æ¸…ç†
                    try:
                        # ç­‰å¾…æ‰€æœ‰æŒ‚èµ·çš„ä»»åŠ¡å®Œæˆ
                        pending = asyncio.all_tasks(loop)
                        if pending:
                            logger.debug(f"ç­‰å¾… {len(pending)} ä¸ªæŒ‚èµ·çš„ä»»åŠ¡å®Œæˆ...")
                            loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
                    except Exception as e:
                        logger.debug(f"æ¸…ç†æŒ‚èµ·ä»»åŠ¡æ—¶å‡ºé”™: {e}")
                    finally:
                        loop.close()
                    
            except Exception as e:
                logger.error(f"å¢é‡æ›´æ–°å¤±è´¥: {e}")
                manager.finish_analysis(analysis_id)
        
        # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        thread = threading.Thread(target=run_async_analysis, daemon=True)
        thread.start()
    
    async def _run_full_analysis(self) -> List[PriceDiffItem]:
        """è¿è¡Œå…¨é‡åˆ†æ"""
        async with IntegratedPriceAnalyzer() as analyzer:
            return await analyzer.analyze_price_differences(
                max_output_items=Config.MAX_OUTPUT_ITEMS
            )
    
    async def _run_incremental_analysis(self) -> List[PriceDiffItem]:
        """è¿è¡ŒçœŸæ­£çš„å¢é‡åˆ†æï¼šæ ¹æ®HashNameç¼“å­˜æœç´¢æœ€æ–°æ•°æ®å¹¶æ›´æ–°å…¨é‡æ–‡ä»¶"""
        hashnames = self.hashname_cache.get_hashnames_for_search()
        if not hashnames:
            logger.warning("ğŸ“­ HashNameç¼“å­˜ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå¢é‡æ›´æ–°")
            return []
        
        logger.info(f"ğŸ” å¼€å§‹å¢é‡æ›´æ–°: æœç´¢ {len(hashnames)} ä¸ªå•†å“çš„æœ€æ–°ä»·æ ¼")
        
        # ğŸ”¥ ç¬¬ä¸€æ­¥ï¼šæ ¹æ®HashNameç¼“å­˜æœç´¢æœ€æ–°æ•°æ®
        updated_items = []
        search_results = {'buff': [], 'youpin': []}
        
        async with SearchManager() as search_manager:
            # é™åˆ¶å¹¶å‘æœç´¢æ•°é‡
            semaphore = asyncio.Semaphore(3)  # é™ä½å¹¶å‘æ•°ï¼Œé¿å…APIé™åˆ¶
            
            async def search_and_collect(keyword):
                async with semaphore:
                    try:
                        # æœç´¢ä¸¤ä¸ªå¹³å°è·å–æœ€æ–°æ•°æ®
                        logger.info(f"ğŸ” å¼€å§‹æœç´¢å…³é”®è¯: {keyword}")
                        results = await search_manager.search_both_platforms(keyword)
                        
                        # ğŸ”¥ ä¿®æ”¹ï¼šæ˜¾ç¤ºä»·æ ¼è€Œä¸æ˜¯æ•°é‡
                        buff_results = results.get('buff', [])
                        youpin_results = results.get('youpin', [])
                        
                        # è·å–æœ€ä½ä»·æ ¼
                        buff_price = f"Â¥{min(item.price for item in buff_results):.2f}" if buff_results else "æ— "
                        youpin_price = f"Â¥{min(item.price for item in youpin_results):.2f}" if youpin_results else "æ— "
                        
                        logger.info(f"ğŸ” æœç´¢ç»“æœ '{keyword}': Buff={buff_price}, æ‚ æ‚ æœ‰å“={youpin_price}")
                        
                        # ğŸ”¥ å¦‚æœæ‚ æ‚ æœ‰å“æœç´¢æ— ç»“æœï¼Œè¾“å‡ºæ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                        if not youpin_results:
                            logger.warning(f"âš ï¸ æ‚ æ‚ æœ‰å“æœç´¢æ— ç»“æœ: {keyword}")
                            logger.info(f"   ğŸ“Š æ‚ æ‚ æœ‰å“åŸå§‹å“åº”æ•°æ®: {results.get('youpin', [])}")
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯APIé”™è¯¯è¿˜æ˜¯çœŸçš„æ²¡æœ‰æ•°æ®
                            if isinstance(results.get('youpin'), list):
                                logger.info(f"   âœ… æ‚ æ‚ æœ‰å“APIè°ƒç”¨æˆåŠŸï¼Œä½†å•†å“åˆ—è¡¨ä¸ºç©º")
                            else:
                                logger.error(f"   âŒ æ‚ æ‚ æœ‰å“APIè°ƒç”¨å¯èƒ½å¤±è´¥")
                        
                        # å¦‚æœæŸä¸ªå¹³å°æœç´¢ç»“æœä¸º0ï¼Œè®°å½•è­¦å‘Š
                        if not buff_results:
                            logger.warning(f"âš ï¸ Buffæœç´¢æ— ç»“æœ: {keyword}")
                            logger.info(f"   ğŸ“Š BuffåŸå§‹å“åº”æ•°æ®: {results.get('buff', [])}")
                        
                        return keyword, results
                        
                    except Exception as e:
                        logger.error(f"ğŸ” å¢é‡æœç´¢å¤±è´¥ {keyword}: {e}")
                        return keyword, {'buff': [], 'youpin': []}
            
            # æ‰¹é‡å¤„ç†ï¼Œé¿å…è¿‡å¤šå¹¶å‘
            batch_size = 5  # å‡å°æ‰¹æ¬¡å¤§å°
            total_updated = 0
            
            for i in range(0, len(hashnames), batch_size):
                batch_keywords = hashnames[i:i + batch_size]
                
                tasks = [search_and_collect(keyword) for keyword in batch_keywords]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # æ”¶é›†æœç´¢ç»“æœ
                for result in batch_results:
                    if isinstance(result, Exception):
                        logger.error(f"æ‰¹é‡æœç´¢å¼‚å¸¸: {result}")
                        continue
                        
                    keyword, results = result
                    if results:
                        # åˆå¹¶æœç´¢ç»“æœ
                        search_results['buff'].extend(results.get('buff', []))
                        search_results['youpin'].extend(results.get('youpin', []))
                        total_updated += len(results.get('buff', [])) + len(results.get('youpin', []))
                
                # è¿›åº¦æŠ¥å‘Š
                logger.info(f"ğŸ”„ å¢é‡æœç´¢è¿›åº¦: {min(i + batch_size, len(hashnames))}/{len(hashnames)}")
                
                # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
                await asyncio.sleep(2)
        
        logger.info(f"ğŸ“Š å¢é‡æœç´¢å®Œæˆ: è·å–åˆ° {len(search_results['buff'])} ä¸ªBuffå•†å“, {len(search_results['youpin'])} ä¸ªæ‚ æ‚ æœ‰å“å•†å“")
        
        # ğŸ”¥ ç¬¬äºŒæ­¥ï¼šæ›´æ–°å…¨é‡æ•°æ®æ–‡ä»¶
        if search_results['buff'] or search_results['youpin']:
            updated_count = await self._update_full_data_files(search_results)
            logger.info(f"ğŸ“ å…¨é‡æ•°æ®æ–‡ä»¶æ›´æ–°å®Œæˆ: {updated_count} ä¸ªå•†å“å·²æ›´æ–°")
        
        # ğŸ”¥ ç¬¬ä¸‰æ­¥ï¼šé‡æ–°åˆ†æä»·å·®
        from saved_data_processor import get_saved_data_processor
        processor = get_saved_data_processor()
        
        if processor.has_valid_full_data():
            logger.info("ğŸ”„ åŸºäºæ›´æ–°åçš„å…¨é‡æ•°æ®é‡æ–°åˆ†æä»·å·®...")
            diff_items, stats = processor.reprocess_with_current_filters()
            
            if diff_items:
                # ğŸ”¥ ç¬¬å››æ­¥ï¼šæ›´æ–°HashNameç¼“å­˜
                self.hashname_cache.update_from_full_analysis(diff_items)
                logger.info(f"ğŸ¯ å¢é‡æ›´æ–°å®Œæˆ: åˆ†æå‡º {len(diff_items)} ä¸ªç¬¦åˆæ¡ä»¶çš„å•†å“")
                return diff_items
            else:
                logger.warning("âš ï¸ é‡æ–°åˆ†æåæœªå‘ç°ç¬¦åˆæ¡ä»¶çš„å•†å“")
        else:
            logger.error("âŒ å…¨é‡æ•°æ®æ–‡ä»¶æ— æ•ˆï¼Œæ— æ³•é‡æ–°åˆ†æ")
        
        return []
        
    async def _update_full_data_files(self, search_results: Dict) -> int:
        """æ›´æ–°å…¨é‡æ•°æ®æ–‡ä»¶ä¸­å¯¹åº”å•†å“çš„æœ€æ–°æ•°æ®"""
        import os
        import json
        from typing import Dict, List
        
        updated_count = 0
        
        # æ›´æ–°Buffæ•°æ®æ–‡ä»¶
        buff_file = "data/buff_full.json"
        if os.path.exists(buff_file) and search_results.get('buff'):
            try:
                # è¯»å–ç°æœ‰æ•°æ®
                with open(buff_file, 'r', encoding='utf-8') as f:
                    buff_data = json.load(f)
                
                # ğŸ”¥ ä¿®å¤ï¼šåˆ›å»ºæ–°æ•°æ®çš„ç´¢å¼•ï¼Œç¡®ä¿å¤„ç†SearchResultå¯¹è±¡
                new_buff_data = {}
                for item in search_results['buff']:
                    if hasattr(item, 'id') and item.id:
                        # ğŸ”¥ è°ƒè¯•ï¼šå°è¯•å¤šç§IDæ ¼å¼
                        item_id = str(item.id)
                        new_buff_data[item_id] = item
                        # ä¹Ÿæ·»åŠ æ•°å­—æ ¼å¼çš„IDï¼ˆå¦‚æœå¯èƒ½ï¼‰
                        try:
                            numeric_id = int(item_id)
                            new_buff_data[str(numeric_id)] = item
                        except ValueError:
                            pass
                
                logger.info(f"ğŸ” å‡†å¤‡æ›´æ–°Buffæ•°æ®: {len(search_results['buff'])} ä¸ªæœç´¢ç»“æœ")
                logger.debug(f"   æœç´¢ç»“æœIDæ ·ä¾‹: {list(new_buff_data.keys())[:5]}")
                
                # ğŸ”¥ è°ƒè¯•ï¼šæ£€æŸ¥å…¨é‡æ•°æ®ç»“æ„
                if isinstance(buff_data, dict) and 'items' in buff_data:
                    items_to_check = buff_data['items']
                    logger.debug(f"   å…¨é‡æ•°æ®ç»“æ„: dict with 'items' key, {len(items_to_check)} ä¸ªå•†å“")
                elif isinstance(buff_data, list):
                    items_to_check = buff_data
                    logger.debug(f"   å…¨é‡æ•°æ®ç»“æ„: list, {len(items_to_check)} ä¸ªå•†å“")
                else:
                    logger.error(f"   âŒ æœªçŸ¥çš„å…¨é‡æ•°æ®ç»“æ„: {type(buff_data)}")
                    items_to_check = []
                
                # æ˜¾ç¤ºå‡ ä¸ªå…¨é‡æ•°æ®IDæ ·ä¾‹
                sample_ids = []
                for i, item in enumerate(items_to_check[:5]):
                    if isinstance(item, dict) and 'id' in item:
                        sample_ids.append(str(item['id']))
                logger.debug(f"   å…¨é‡æ•°æ®IDæ ·ä¾‹: {sample_ids}")
                
                # æ›´æ–°ç°æœ‰æ•°æ®
                items_updated = 0
                checked_count = 0
                for i, item in enumerate(items_to_check):
                    if isinstance(item, dict):  # ç¡®ä¿itemæ˜¯å­—å…¸
                        item_id = str(item.get('id', ''))  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡ŒåŒ¹é…
                        checked_count += 1
                        
                        if item_id in new_buff_data:
                            new_item = new_buff_data[item_id]
                            # æ›´æ–°å…³é”®å­—æ®µ
                            old_price = item.get('sell_min_price', item.get('price', 0))
                            item['sell_min_price'] = float(new_item.price)  # ğŸ”¥ ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
                            if hasattr(new_item, 'sell_num') and new_item.sell_num is not None:
                                item['sell_num'] = int(new_item.sell_num)
                            item['last_updated'] = datetime.now().isoformat()
                            items_updated += 1
                            logger.debug(f"âœ… æ›´æ–°å•†å“ID {item_id}: {item.get('name', 'Unknown')} - ä»·æ ¼: {old_price} -> {new_item.price}")
                        elif checked_count <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ªæœªåŒ¹é…çš„ID
                            logger.debug(f"âŒ ID {item_id} æœªåœ¨æœç´¢ç»“æœä¸­æ‰¾åˆ°åŒ¹é…")
                
                logger.info(f"ğŸ” IDåŒ¹é…ç»Ÿè®¡: æ£€æŸ¥äº† {checked_count} ä¸ªå…¨é‡å•†å“, åŒ¹é…åˆ° {items_updated} ä¸ª")
                
                # ä¿å­˜æ›´æ–°åçš„æ•°æ®
                with open(buff_file, 'w', encoding='utf-8') as f:
                    json.dump(buff_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"ğŸ“ Buffæ•°æ®æ–‡ä»¶å·²æ›´æ–°: {items_updated} ä¸ªå•†å“")
                updated_count += items_updated
                
            except Exception as e:
                logger.error(f"âŒ æ›´æ–°Buffæ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
                logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        
        # æ›´æ–°æ‚ æ‚ æœ‰å“æ•°æ®æ–‡ä»¶
        youpin_file = "data/youpin_full.json"
        if os.path.exists(youpin_file) and search_results.get('youpin'):
            try:
                # è¯»å–ç°æœ‰æ•°æ®
                with open(youpin_file, 'r', encoding='utf-8') as f:
                    youpin_data = json.load(f)
                
                # ğŸ”¥ ä¿®å¤ï¼šåˆ›å»ºæ–°æ•°æ®çš„ç´¢å¼• (ä½¿ç”¨nameä½œä¸ºé”®ï¼Œå› ä¸ºæ‚ æ‚ æœ‰å“å¯èƒ½æ²¡æœ‰id)
                new_youpin_data = {}
                for item in search_results['youpin']:
                    if hasattr(item, 'id') and item.id:
                        key = str(item.id)
                        new_youpin_data[key] = item
                    if hasattr(item, 'name') and item.name:
                        # ä¹Ÿç”¨nameä½œä¸ºé”®
                        new_youpin_data[item.name] = item
                
                logger.info(f"ğŸ” å‡†å¤‡æ›´æ–°æ‚ æ‚ æœ‰å“æ•°æ®: {len(search_results['youpin'])} ä¸ªæœç´¢ç»“æœ")
                logger.debug(f"   æ‚ æ‚ æœ‰å“æœç´¢ç»“æœé”®æ ·ä¾‹: {list(new_youpin_data.keys())[:5]}")
                
                # ğŸ”¥ è°ƒè¯•ï¼šæ£€æŸ¥å…¨é‡æ•°æ®ç»“æ„
                if isinstance(youpin_data, dict) and 'items' in youpin_data:
                    items_to_check = youpin_data['items']
                elif isinstance(youpin_data, list):
                    items_to_check = youpin_data
                else:
                    logger.error(f"   âŒ æœªçŸ¥çš„æ‚ æ‚ æœ‰å“æ•°æ®ç»“æ„: {type(youpin_data)}")
                    items_to_check = []
                
                # æ›´æ–°ç°æœ‰æ•°æ®
                items_updated = 0
                checked_count = 0
                for i, item in enumerate(items_to_check):
                    if isinstance(item, dict):  # ç¡®ä¿itemæ˜¯å­—å…¸
                        checked_count += 1
                        # å°è¯•ç”¨idåŒ¹é…ï¼Œå¦‚æœæ²¡æœ‰idåˆ™ç”¨nameåŒ¹é…
                        item_key = str(item.get('id', '')) if item.get('id') else item.get('name', '')
                        if item_key and item_key in new_youpin_data:
                            new_item = new_youpin_data[item_key]
                            # æ›´æ–°å…³é”®å­—æ®µ
                            old_price = item.get('price', 0)
                            item['price'] = float(new_item.price)
                            item['last_updated'] = datetime.now().isoformat()
                            items_updated += 1
                            logger.debug(f"âœ… æ›´æ–°æ‚ æ‚ æœ‰å“å•†å“ {item_key}: {item.get('name', 'Unknown')} - ä»·æ ¼: {old_price} -> {new_item.price}")
                        elif checked_count <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ªæœªåŒ¹é…çš„
                            logger.debug(f"âŒ æ‚ æ‚ æœ‰å“é”® {item_key} æœªæ‰¾åˆ°åŒ¹é…")
                
                logger.info(f"ğŸ” æ‚ æ‚ æœ‰å“åŒ¹é…ç»Ÿè®¡: æ£€æŸ¥äº† {checked_count} ä¸ªå…¨é‡å•†å“, åŒ¹é…åˆ° {items_updated} ä¸ª")
                
                # ä¿å­˜æ›´æ–°åçš„æ•°æ®
                with open(youpin_file, 'w', encoding='utf-8') as f:
                    json.dump(youpin_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"ğŸ“ æ‚ æ‚ æœ‰å“æ•°æ®æ–‡ä»¶å·²æ›´æ–°: {items_updated} ä¸ªå•†å“")
                updated_count += items_updated
                
            except Exception as e:
                logger.error(f"âŒ æ›´æ–°æ‚ æ‚ æœ‰å“æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
                logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        
        return updated_count
    
    def _analyze_search_results(self, search_results: Dict) -> List[PriceDiffItem]:
        """åˆ†ææœç´¢ç»“æœï¼Œè®¡ç®—ä»·å·®"""
        diff_items = []
        
        youpin_results = search_results.get('youpin', [])
        buff_results = search_results.get('buff', [])
        
        # æŒ‰hash_nameåŒ¹é…ä»·æ ¼
        youpin_prices = {item.hash_name: item for item in youpin_results if item.hash_name}
        buff_prices = {item.hash_name: item for item in buff_results if item.hash_name}
        
        # å¦‚æœæ²¡æœ‰hash_nameï¼Œä½¿ç”¨nameåŒ¹é…
        if not youpin_prices:
            youpin_prices = {item.name: item for item in youpin_results}
        if not buff_prices:
            buff_prices = {item.name: item for item in buff_results}
        
        # åŒ¹é…å¹¶è®¡ç®—ä»·å·®
        for hash_name, buff_item in buff_prices.items():
            youpin_item = youpin_prices.get(hash_name)
            
            if youpin_item and buff_item.price > 0 and youpin_item.price > 0:
                # æ£€æŸ¥Buffä»·æ ¼æ˜¯å¦åœ¨ç­›é€‰èŒƒå›´å†…
                if not Config.is_buff_price_in_range(buff_item.price):
                    continue
                
                # ğŸ”¥ æ–°å¢ï¼šæ£€æŸ¥Buffåœ¨å”®æ•°é‡æ˜¯å¦ç¬¦åˆæ¡ä»¶
                if hasattr(buff_item, 'sell_num') and buff_item.sell_num is not None:
                    if not Config.is_buff_sell_num_valid(buff_item.sell_num):
                        continue
                
                price_diff = youpin_item.price - buff_item.price
                
                # æ£€æŸ¥ä»·å·®æ˜¯å¦ç¬¦åˆè¦æ±‚
                if Config.is_price_diff_in_range(price_diff):
                    profit_rate = (price_diff / buff_item.price) * 100 if buff_item.price > 0 else 0
                    
                    # ğŸ”¥ ä¿®å¤ï¼šæå–hash_nameï¼Œä¼˜å…ˆä»buff_itemè·å–
                    hash_name = getattr(buff_item, 'hash_name', None) or getattr(buff_item, 'market_hash_name', None) or buff_item.name
                    
                    diff_item = PriceDiffItem(
                        id=buff_item.id,
                        name=buff_item.name,
                        hash_name=hash_name,  # ğŸ”¥ æ–°å¢hash_nameå­—æ®µ
                        buff_price=buff_item.price,
                        youpin_price=youpin_item.price,
                        price_diff=price_diff,
                        profit_rate=profit_rate,
                        buff_url=buff_item.market_url,
                        youpin_url=youpin_item.market_url,
                        image_url=buff_item.image_url,
                        category="æœç´¢ç»“æœ",
                        last_updated=datetime.now()
                    )
                    
                    diff_items.append(diff_item)
        
        return diff_items
    
    def _merge_incremental_data(self, incremental_items: List[PriceDiffItem]):
        """åˆå¹¶å¢é‡æ•°æ®åˆ°å½“å‰æ•°æ®ä¸­"""
        # åˆ›å»ºå½“å‰æ•°æ®çš„ç´¢å¼•ï¼ˆæŒ‰nameæˆ–idï¼‰
        current_index = {}
        for i, item in enumerate(self.current_diff_items):
            key = f"{item.name}_{item.id}" if item.id else item.name
            current_index[key] = i
        
        # åˆå¹¶æ–°æ•°æ®
        for new_item in incremental_items:
            key = f"{new_item.name}_{new_item.id}" if new_item.id else new_item.name
            
            if key in current_index:
                # æ›´æ–°ç°æœ‰å•†å“
                self.current_diff_items[current_index[key]] = new_item
            else:
                # æ·»åŠ æ–°å•†å“
                self.current_diff_items.append(new_item)
        
        # æŒ‰ä»·å·®æ’åº
        self.current_diff_items.sort(key=lambda x: x.price_diff, reverse=True)
        
        # é™åˆ¶æ•°é‡
        if len(self.current_diff_items) > Config.MAX_OUTPUT_ITEMS:
            self.current_diff_items = self.current_diff_items[:Config.MAX_OUTPUT_ITEMS]
    
    def get_current_data(self) -> List[PriceDiffItem]:
        """è·å–å½“å‰æ•°æ®"""
        return self.current_diff_items.copy()
    
    def get_status(self) -> Dict:
        """è·å–æ›´æ–°çŠ¶æ€"""
        return {
            'is_running': self.is_running,
            'initial_full_update_completed': self.initial_full_update_completed,
            'last_full_update': self.last_full_update.isoformat() if self.last_full_update else None,
            'last_incremental_update': self.last_incremental_update.isoformat() if self.last_incremental_update else None,
            'current_items_count': len(self.current_diff_items),
            'cached_hashnames_count': len(self.hashname_cache.hashname_profits),
            'should_full_update': self.hashname_cache.should_full_update(),
            'full_update_interval_hours': Config.FULL_UPDATE_INTERVAL_HOURS,
            'incremental_update_interval_minutes': Config.INCREMENTAL_UPDATE_INTERVAL_MINUTES
        }
    
    def force_full_update(self):
        """å¼ºåˆ¶å…¨é‡æ›´æ–°"""
        logger.info("ğŸ”„ å¼ºåˆ¶æ‰§è¡Œå…¨é‡æ›´æ–°")
        # ğŸ”¥ ä¿®å¤ï¼šç›´æ¥è°ƒç”¨è€Œä¸æ˜¯åˆ›å»ºæ–°çº¿ç¨‹ï¼Œé¿å…å¤šä¸ªçº¿ç¨‹åŒæ—¶è¿è¡Œ
        self._trigger_full_update()
    
    def force_incremental_update(self):
        """å¼ºåˆ¶å¢é‡æ›´æ–°"""
        logger.info("ğŸ”„ å¼ºåˆ¶æ‰§è¡Œå¢é‡æ›´æ–°")
        threading.Thread(target=self._trigger_incremental_update, daemon=True).start()
    
    def _load_latest_data(self):
        """åŠ è½½æœ€æ–°çš„ä»·å·®æ•°æ®"""
        try:
            import os  # ğŸ”¥ ç¡®ä¿osæ¨¡å—å¯ç”¨
            # å°è¯•åŠ è½½ä¿å­˜çš„ä»·å·®æ•°æ®
            if os.path.exists(Config.LATEST_DATA_FILE):
                with open(Config.LATEST_DATA_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # è½¬æ¢ä¸ºPriceDiffItemå¯¹è±¡
                loaded_items = []
                for item_data in data.get('items', []):
                    try:
                        item = PriceDiffItem(
                            id=item_data.get('id', ''),
                            name=item_data.get('name', ''),
                            hash_name=item_data.get('hash_name', item_data.get('name', '')),  # ğŸ”¥ æ–°å¢hash_nameå­—æ®µï¼Œå…¼å®¹æ—§æ•°æ®
                            buff_price=float(item_data.get('buff_price', 0)),
                            youpin_price=float(item_data.get('youpin_price', 0)),
                            price_diff=float(item_data.get('price_diff', 0)),
                            profit_rate=float(item_data.get('profit_rate', 0)),
                            buff_url=item_data.get('buff_url', ''),
                            youpin_url=item_data.get('youpin_url', ''),
                            image_url=item_data.get('image_url', ''),
                            category=item_data.get('category', ''),
                            last_updated=datetime.fromisoformat(item_data['last_updated']) if item_data.get('last_updated') else datetime.now()
                        )
                        loaded_items.append(item)
                    except Exception as e:
                        logger.warning(f"è§£æä¿å­˜çš„å•†å“æ•°æ®å¤±è´¥: {e}")
                        continue
                
                if loaded_items:
                    self.current_diff_items = loaded_items
                    # ä»æ–‡ä»¶å…ƒæ•°æ®è·å–æ›´æ–°æ—¶é—´
                    metadata = data.get('metadata', {})
                    if metadata.get('last_full_update'):
                        self.last_full_update = datetime.fromisoformat(metadata['last_full_update'])
                    
                    logger.info(f"ğŸ“Š å·²åŠ è½½ç¼“å­˜æ•°æ®: {len(loaded_items)}ä¸ªå•†å“")
                else:
                    logger.warning("ç¼“å­˜æ•°æ®æ–‡ä»¶ä¸ºç©º")
            else:
                logger.info("æœªæ‰¾åˆ°ç¼“å­˜æ•°æ®æ–‡ä»¶")
                
        except Exception as e:
            logger.error(f"åŠ è½½æœ€æ–°æ•°æ®å¤±è´¥: {e}")
    
    def _save_current_data(self):
        """ä¿å­˜å½“å‰æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(Config.LATEST_DATA_FILE), exist_ok=True)
            
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            items_data = []
            for item in self.current_diff_items:
                items_data.append({
                    'id': item.id,
                    'name': item.name,
                    'hash_name': getattr(item, 'hash_name', item.name),  # ğŸ”¥ æ–°å¢hash_nameå­—æ®µï¼Œå…¼å®¹æ—§æ•°æ®
                    'buff_price': item.buff_price,
                    'youpin_price': item.youpin_price,
                    'price_diff': item.price_diff,
                    'profit_rate': item.profit_rate,
                    'buff_url': item.buff_url,
                    'youpin_url': item.youpin_url,
                    'image_url': item.image_url,
                    'category': item.category,
                    'last_updated': item.last_updated.isoformat() if item.last_updated else None
                })
            
            data = {
                'metadata': {
                    'last_full_update': self.last_full_update.isoformat() if self.last_full_update else None,
                    'last_incremental_update': self.last_incremental_update.isoformat() if self.last_incremental_update else None,
                    'total_count': len(items_data),
                    'generated_at': datetime.now().isoformat()
                },
                'items': items_data
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(Config.LATEST_DATA_FILE, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"ğŸ’¾ å·²ä¿å­˜ {len(items_data)} ä¸ªå•†å“åˆ°ç¼“å­˜æ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"ä¿å­˜å½“å‰æ•°æ®å¤±è´¥: {e}")

    def _regenerate_cache_from_full_data(self) -> bool:
        """
        ä»full dataæ–‡ä»¶é‡æ–°ç”Ÿæˆhash nameç¼“å­˜
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸé‡æ–°ç”Ÿæˆç¼“å­˜
        """
        try:
            import os
            import json
            
            buff_file = "data/buff_full.json"
            youpin_file = "data/youpin_full.json"
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(buff_file) or not os.path.exists(youpin_file):
                logger.info("ğŸ” æœªæ‰¾åˆ°full dataæ–‡ä»¶ï¼Œè·³è¿‡ç¼“å­˜é‡æ–°ç”Ÿæˆ")
                return False
            
            logger.info("ğŸ” å‘ç°full dataæ–‡ä»¶ï¼Œå¼€å§‹é‡æ–°ç”Ÿæˆhash nameç¼“å­˜...")
            
            # è¯»å–æ•°æ®æ–‡ä»¶
            with open(buff_file, 'r', encoding='utf-8') as f:
                buff_data = json.load(f)
            
            with open(youpin_file, 'r', encoding='utf-8') as f:
                youpin_data = json.load(f)
            
            buff_items = buff_data.get('items', [])
            youpin_items = youpin_data.get('items', [])
            
            logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®: Buff {len(buff_items)}ä¸ªå•†å“, æ‚ æ‚ æœ‰å“ {len(youpin_items)}ä¸ªå•†å“")
            
            # ä½¿ç”¨saved_data_processorè¿›è¡Œå¿«é€Ÿåˆ†æ
            from saved_data_processor import get_saved_data_processor
            processor = get_saved_data_processor()
            
            # åˆ†æå¹¶ç­›é€‰æœ‰ä»·å·®çš„å•†å“
            diff_items, stats = processor._analyze_with_current_filters(buff_items, youpin_items)
            
            if diff_items:
                logger.info(f"ğŸ¯ åˆ†æå®Œæˆ: å‘ç° {len(diff_items)} ä¸ªæœ‰ä»·å·®çš„å•†å“")
                
                # æ›´æ–°ç¼“å­˜å’Œå½“å‰æ•°æ®
                self.hashname_cache.update_from_full_analysis(diff_items)
                self.current_diff_items = diff_items
                self._save_current_data()
                
                # æ›´æ–°æ—¶é—´æˆ³
                self.last_full_update = datetime.now()
                
                logger.info("âœ… HashNameç¼“å­˜å·²ä»full dataæ–‡ä»¶é‡æ–°ç”Ÿæˆ")
                return True
            else:
                logger.warning("âš ï¸ æœªå‘ç°æœ‰ä»·å·®çš„å•†å“ï¼Œæ— æ³•ç”Ÿæˆç¼“å­˜")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ä»full dataæ–‡ä»¶é‡æ–°ç”Ÿæˆç¼“å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

# å…¨å±€æ›´æ–°ç®¡ç†å™¨å®ä¾‹
_update_manager_instance = None

def get_update_manager() -> UpdateManager:
    """è·å–æ›´æ–°ç®¡ç†å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _update_manager_instance
    if _update_manager_instance is None:
        _update_manager_instance = UpdateManager()
    return _update_manager_instance

# æµ‹è¯•åŠŸèƒ½
async def test_update_manager():
    """æµ‹è¯•æ›´æ–°ç®¡ç†å™¨"""
    print("ğŸ§ª æµ‹è¯•æ›´æ–°ç®¡ç†å™¨")
    print("="*50)
    
    manager = get_update_manager()
    
    # æ˜¾ç¤ºçŠ¶æ€
    status = manager.get_status()
    print(f"çŠ¶æ€: {status}")
    
    # å¼ºåˆ¶æ‰§è¡Œä¸€æ¬¡å…¨é‡æ›´æ–°
    print("\nğŸ”„ æ‰§è¡Œå…¨é‡æ›´æ–°æµ‹è¯•...")
    manager.force_full_update()
    
    # ç­‰å¾…ä¸€æ®µæ—¶é—´
    await asyncio.sleep(30)
    
    # æ˜¾ç¤ºç»“æœ
    data = manager.get_current_data()
    print(f"å½“å‰æ•°æ®: {len(data)}ä¸ªå•†å“")
    
    if data:
        print("å‰3ä¸ªå•†å“:")
        for i, item in enumerate(data[:3], 1):
            print(f"  {i}. {item.name}: ä»·å·®Â¥{item.price_diff:.2f}")

if __name__ == "__main__":
    asyncio.run(test_update_manager()) 